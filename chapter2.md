## 第2章：神经网络计算基础

要设计高效的NPU，必须深入理解神经网络的计算本质。本章将从硬件设计者的视角，详细分析神经网络的基本运算、数据流特征和优化机会。通过对计算模式的深入剖析，我们能够识别出硬件加速的关键点，为后续的NPU架构设计奠定基础。

神经网络的计算看似复杂，但如果我们剥开层层抽象，会发现其核心是高度规律的数学运算。一个训练好的GPT-3模型包含1750亿个参数，执行一次推理需要进行数千亿次乘加运算，但这些运算的模式却惊人地一致。这种"规律性"正是硬件加速的黄金机会——我们可以设计专门的电路来高效执行这些重复的运算模式。

本章将带你深入理解神经网络计算的本质，从最基础的神经元模型开始，逐步扩展到矩阵运算、卷积操作，再到现代Transformer架构的注意力机制。更重要的是，我们将探讨如何在脉动阵列和数据流架构中高效实现这些运算，以及量化、稀疏化等优化技术如何在保持精度的同时大幅降低计算复杂度。通过本章的学习，你将建立起从算法到硬件的完整认知链条。

### 2.1 神经网络基本运算

神经网络虽然结构复杂，但其底层运算却相对简单和规律。这种"复杂系统由简单元素构成"的特性，正是硬件加速的机会所在。通过对基本运算的深入分析，我们可以设计出高效的硬件加速单元。

#### 2.1.1 神经元计算模型

神经元是神经网络的基本计算单元，其灵感来源于生物神经元。从数学角度看，一个神经元执行的是加权求和后的非线性变换。虽然概念简单，但当数百万个神经元协同工作时，就能展现出强大的学习和推理能力。

人工神经元的数学模型可以表示为：

y = f(Σ(wi * xi) + b)

其中：
- xi：输入信号（来自上一层神经元的输出）
- wi：连接权重（通过学习得到的参数）
- wi * xi：加权输入 (Weighted Input)
- Σ(...)：对所有输入的求和 (Summation)  
- b：偏置项 (Bias)，用于调节神经元的激活阈值
- f(...)：激活函数 (Activation Function)，引入非线性
- y：神经元的输出
