<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>第10章：软件栈与编译优化 - NPU设计教程</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Microsoft YaHei', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f5;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: linear-gradient(135deg, #2c3e50 0%, #3498db 100%);
            color: white;
            padding: 40px 0;
            text-align: center;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        header h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }

        .nav-bar {
            background: #34495e;
            padding: 15px 0;
            position: sticky;
            top: 0;
            z-index: 100;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }

        .nav-bar ul {
            list-style: none;
            display: flex;
            justify-content: center;
            max-width: 1200px;
            margin: 0 auto;
            padding: 0;
        }

        .nav-bar li {
            margin: 0 15px;
        }

        .nav-bar a {
            color: white;
            text-decoration: none;
            padding: 5px 10px;
            border-radius: 4px;
            transition: background 0.3s;
        }

        .nav-bar a:hover {
            background: #2c3e50;
        }

        .nav-bar .current {
            background: #2c3e50;
            font-weight: bold;
        }

        .chapter {
            background: white;
            margin: 20px 0;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }

        .chapter h2 {
            color: #2c3e50;
            font-size: 2em;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #3498db;
        }

        .chapter h3 {
            color: #34495e;
            font-size: 1.5em;
            margin: 25px 0 15px 0;
        }

        .chapter h4 {
            color: #7f8c8d;
            font-size: 1.2em;
            margin: 20px 0 10px 0;
        }

        .code-block {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 20px;
            border-radius: 5px;
            margin: 15px 0;
            overflow-x: auto;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 14px;
            line-height: 1.5;
            white-space: pre-wrap;
            word-wrap: break-word;
            position: relative;
        }
        
        /* Language label */
        .code-block::before {
            content: attr(data-language);
            position: absolute;
            top: 5px;
            right: 10px;
            font-size: 12px;
            color: #95a5a6;
            text-transform: uppercase;
        }
        
        /* Syntax highlighting classes */
        .code-block .keyword { color: #e74c3c; font-weight: bold; }
        .code-block .type { color: #3498db; }
        .code-block .comment { color: #95a5a6; font-style: italic; }
        .code-block .number { color: #e67e22; }
        .code-block .string { color: #2ecc71; }
        .code-block .function { color: #3498db; }

        .exercise {
            background: #ecf0f1;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
            border-left: 5px solid #3498db;
        }

        .exercise h4 {
            color: #2c3e50;
            margin-bottom: 10px;
        }

        .question {
            margin: 15px 0;
            padding: 15px;
            background: white;
            border-radius: 5px;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }

        .answer {
            margin-top: 10px;
            padding: 15px;
            background: #e8f5e9;
            border-radius: 5px;
            display: none;
            border-left: 4px solid #4caf50;
        }

        .answer.show {
            display: block;
        }
        
        .hint {
            margin: 10px 0;
            padding: 10px 15px;
            background: #fff8dc;
            border-left: 4px solid #ffa500;
            border-radius: 5px;
            font-size: 0.95em;
        }
        
        .hint summary {
            cursor: pointer;
            font-weight: bold;
            color: #ff8c00;
            outline: none;
        }
        
        .hint summary:hover {
            color: #ff6347;
        }
        
        .hint p {
            margin-top: 10px;
            color: #666;
        }

        .toggle-answer {
            background: #3498db;
            color: white;
            border: none;
            padding: 8px 15px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            margin-top: 10px;
            transition: background 0.3s;
        }

        .toggle-answer:hover {
            background: #2980b9;
        }

        .table-wrapper {
            overflow-x: auto;
            margin: 20px 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }

        th {
            background: #34495e;
            color: white;
            font-weight: bold;
        }

        tr:hover {
            background: #f5f5f5;
        }

        .info-box {
            background: #e3f2fd;
            border-left: 5px solid #2196f3;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .warning-box {
            background: #fff3cd;
            border-left: 5px solid #ff9800;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }

        .chapter-nav {
            display: flex;
            justify-content: space-between;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 2px solid #ecf0f1;
        }

        .chapter-nav a {
            background: #3498db;
            color: white;
            padding: 10px 20px;
            text-decoration: none;
            border-radius: 5px;
            transition: background 0.3s;
        }

        .chapter-nav a:hover {
            background: #2980b9;
        }

        .chapter-nav .prev::before {
            content: "← ";
        }

        .chapter-nav .next::after {
            content: " →";
        }

        /* Mobile Responsive Design */
        @media (max-width: 768px) {
            .container {
                padding: 10px;
            }
            
            header {
                padding: 20px 10px;
            }
            
            header h1 {
                font-size: 1.5em;
            }
            
            .chapter {
                padding: 15px;
                margin: 10px 0;
            }
            
            .chapter h2 {
                font-size: 1.5em;
            }
            
            .chapter h3 {
                font-size: 1.2em;
            }
            
            .nav-bar ul {
                flex-wrap: wrap;
                justify-content: center;
            }
            
            .nav-bar li {
                margin: 5px;
            }
            
            .code-block {
                padding: 10px;
                font-size: 12px;
            }
            
            table {
                font-size: 14px;
            }
            
            th, td {
                padding: 8px;
            }
        }

        /* List styles for proper indentation */
        .chapter ul, .chapter ol {
            margin-left: 20px;
            margin-bottom: 15px;
        }
        
        .chapter li {
            margin-bottom: 8px;
            line-height: 1.8;
        }
        
        .chapter ul ul, .chapter ol ol, .chapter ul ol, .chapter ol ul {
            margin-left: 20px;
            margin-top: 5px;
        }
        
        .info-box ul, .warning-box ul, .answer ul {
            margin-left: 20px;
        }
        
        .info-box li, .warning-box li, .answer li {
            margin-bottom: 10px;
        }
        
        /* Keep nav-bar lists unstyled */
        .nav-bar ul {
            margin-left: 0;
        }
        
        .nav-bar li {
            margin-bottom: 0;
        }
    </style>
    <script>
        // Syntax highlighting functions
        function escapeHtml(text) {
            const map = {
                '&': '&amp;',
                '<': '&lt;',
                '>': '&gt;',
                '"': '&quot;',
                "'": '&#039;'
            };
            return text.replace(/[&<>"']/g, m => map[m]);
        }
        
        function highlightSyntax() {
            const codeBlocks = document.querySelectorAll('.code-block');
            
            codeBlocks.forEach(block => {
                const content = block.textContent;
                let language = 'text';
                let highlighted = content;
                
                // Auto-detect language based on content
                if (content.includes('module ') || content.includes('always @') || content.includes('wire ') || content.includes('reg ')) {
                    language = 'verilog';
                    highlighted = highlightVerilog(content);
                } else if (content.includes('import ') || content.includes('def ') || content.includes('class ')) {
                    language = 'python';
                    highlighted = highlightPython(content);
                }
                
                block.innerHTML = highlighted;
                block.classList.add(language);
                block.setAttribute('data-language', language);
            });
        }
        
        function highlightVerilog(code) {
            const placeholders = [];
            let placeholderIndex = 0;
            
            // Replace comments with placeholders
            code = code.replace(/(\/\/.*$|\/\*[\s\S]*?\*\/)/gm, (match) => {
                const placeholder = `__COMMENT_${placeholderIndex}__`;
                placeholders[placeholderIndex] = `<span class="comment">${escapeHtml(match)}</span>`;
                placeholderIndex++;
                return placeholder;
            });
            
            // Replace strings with placeholders
            code = code.replace(/("[^"]*")/g, (match) => {
                const placeholder = `__STRING_${placeholderIndex}__`;
                placeholders[placeholderIndex] = `<span class="string">${escapeHtml(match)}</span>`;
                placeholderIndex++;
                return placeholder;
            });
            
            // Apply highlights
            const keywords = /\b(module|endmodule|input|output|wire|reg|always|assign|begin|end|if|else|for|while|parameter|posedge|negedge)\b/g;
            const types = /\b(bit|logic|byte|shortint|int|longint|integer|time|real)\b/g;
            const numbers = /\b(\d+'[hbdo][\da-fA-F_]+|\d+)\b/g;
            
            code = code.replace(keywords, '<span class="keyword">$1</span>');
            code = code.replace(types, '<span class="type">$1</span>');
            code = code.replace(numbers, '<span class="number">$1</span>');
            
            // Restore placeholders
            for (let i = 0; i < placeholderIndex; i++) {
                code = code.replace(new RegExp(`__COMMENT_${i}__`, 'g'), placeholders[i]);
                code = code.replace(new RegExp(`__STRING_${i}__`, 'g'), placeholders[i]);
            }
            
            return code;
        }
        
        function highlightPython(code) {
            const placeholders = [];
            let placeholderIndex = 0;
            
            // Replace comments
            code = code.replace(/(#.*$)/gm, (match) => {
                const placeholder = `__COMMENT_${placeholderIndex}__`;
                placeholders[placeholderIndex] = `<span class="comment">${escapeHtml(match)}</span>`;
                placeholderIndex++;
                return placeholder;
            });
            
            // Replace strings
            code = code.replace(/("[^"]*"|'[^']*')/g, (match) => {
                const placeholder = `__STRING_${placeholderIndex}__`;
                placeholders[placeholderIndex] = `<span class="string">${escapeHtml(match)}</span>`;
                placeholderIndex++;
                return placeholder;
            });
            
            // Apply highlights
            const keywords = /\b(and|as|assert|break|class|continue|def|del|elif|else|except|False|finally|for|from|global|if|import|in|is|lambda|None|not|or|pass|raise|return|True|try|while|with|yield)\b/g;
            const builtins = /\b(abs|all|any|bin|bool|dict|float|format|hex|input|int|len|list|map|max|min|open|print|range|round|set|sorted|str|sum|tuple|type|zip)\b/g;
            const numbers = /\b(\d+\.?\d*)\b/g;
            
            code = code.replace(keywords, '<span class="keyword">$1</span>');
            code = code.replace(builtins, '<span class="function">$1</span>');
            code = code.replace(numbers, '<span class="number">$1</span>');
            
            // Restore placeholders
            for (let i = 0; i < placeholderIndex; i++) {
                code = code.replace(new RegExp(`__COMMENT_${i}__`, 'g'), placeholders[i]);
                code = code.replace(new RegExp(`__STRING_${i}__`, 'g'), placeholders[i]);
            }
            
            return code;
        }
        
        // Toggle answer visibility
        document.addEventListener('DOMContentLoaded', function() {
            highlightSyntax();
            
            const toggleButtons = document.querySelectorAll('.toggle-answer');
            toggleButtons.forEach(button => {
                button.addEventListener('click', function() {
                    const answer = this.nextElementSibling;
                    answer.classList.toggle('show');
                    this.textContent = answer.classList.contains('show') ? '隐藏答案' : '显示答案';
                });
            });
        });
    </script>
</head>
<body>
    <header>
        <h1>第10章：软件栈与编译优化</h1>
    </header>
    
    <nav class="nav-bar">
        <ul>
            <li><a href="index.html">首页</a></li>
            <li><a href="chapter1.html">第1章</a></li>
            <li><a href="chapter2.html">第2章</a></li>
            <li><a href="chapter3.html">第3章</a></li>
            <li><a href="chapter4.html">第4章</a></li>
            <li><a href="chapter5.html">第5章</a></li>
            <li><a href="chapter6.html">第6章</a></li>
            <li><a href="chapter7.html">第7章</a></li>
            <li><a href="chapter8.html">第8章</a></li>
            <li><a href="chapter9.html">第9章</a></li>
            <li><a href="chapter10.html" class="current">第10章</a></li>
            <li><a href="chapter11.html">第11章</a></li>
            <li><a href="chapter12.html">第12章</a></li>
        </ul>
    </nav>
    
    <div class="container">
        <div class="chapter">
            <h2>第10章：软件栈与编译优化</h2>
            
            <p>NPU的硬件性能再强，也需要优秀的软件栈才能充分发挥。本章深入探讨NPU软件栈的架构设计、编译优化技术，以及如何实现高效的软硬件协同。</p>

            <h3>10.1 NPU软件栈架构</h3>
            
            <p>NPU软件栈是连接上层AI框架和底层硬件的桥梁。一个完整的软件栈需要处理模型解析、图优化、算子映射、内存管理、指令生成等复杂任务。</p>

            <h4>10.1.1 软件栈分层架构</h4>
            <div class="code-block">
// NPU软件栈典型架构
┌─────────────────────────────────────────┐
│      AI Frameworks (TensorFlow/PyTorch) │
├─────────────────────────────────────────┤
│         Graph Representation            │
│         (ONNX, TorchScript)            │
├─────────────────────────────────────────┤
│         High-Level IR (HIR)            │
│     (Graph Optimization Pass)          │
├─────────────────────────────────────────┤
│         Mid-Level IR (MIR)             │
│    (Operator Fusion, Tiling)          │
├─────────────────────────────────────────┤
│         Low-Level IR (LIR)             │
│   (Memory Allocation, Scheduling)      │
├─────────────────────────────────────────┤
│      Code Generation Backend           │
│    (NPU Instruction Generation)        │
├─────────────────────────────────────────┤
│         Runtime Library                │
│    (Execution, Memory Management)      │
├─────────────────────────────────────────┤
│         NPU Hardware                   │
└─────────────────────────────────────────┘
            </div>

            <h4>10.1.2 关键组件功能</h4>
            <div class="info-box">
                <p><strong>软件栈核心组件：</strong></p>
                <ul>
                    <li><strong>前端解析器：</strong>支持多种框架模型格式，转换为统一的内部表示</li>
                    <li><strong>图优化器：</strong>执行算子融合、常量折叠、死代码消除等优化</li>
                    <li><strong>量化工具：</strong>支持训练后量化和量化感知训练</li>
                    <li><strong>内存分配器：</strong>优化片上内存使用，最小化数据搬移</li>
                    <li><strong>指令调度器：</strong>生成高效的指令序列，最大化硬件利用率</li>
                    <li><strong>运行时系统：</strong>管理任务执行、内存管理、多核调度</li>
                </ul>
            </div>

            <h4>10.1.3 核心枢纽：中间表示（IR）</h4>
            <p>中间表示（Intermediate Representation, IR）是现代AI编译器的灵魂，它在前端（框架模型）和后端（硬件指令）之间架起桥梁。NPU编译器通常采用多层IR设计，每层针对不同的优化目标。</p>
            
            <div class="code-block">
// 多层IR架构示例
// 1. Graph IR - 高层计算图表示
class GraphIR {
    // 节点表示算子
    struct Node {
        string op_type;        // "Conv2D", "MatMul", "Add", etc.
        vector<Tensor> inputs;
        vector<Tensor> outputs;
        map<string, Attribute> attrs;  // kernel_size, stride, etc.
    };
    
    // 边表示数据流
    struct Edge {
        Node* src;
        Node* dst;
        int src_output_idx;
        int dst_input_idx;
    };
};

// 2. Tensor IR - 张量程序表示
class TensorIR {
    // 类似TVM的张量表达式
    Tensor conv2d_tir(Tensor input, Tensor weight) {
        // 定义计算维度
        auto N = input.shape[0];
        auto H = input.shape[1];
        auto W = input.shape[2];
        auto C = input.shape[3];
        auto K = weight.shape[0];
        
        // 定义输出张量
        Tensor output({N, H-2, W-2, K});
        
        // 定义计算
        output(n, h, w, k) = sum(
            input(n, h+rh, w+rw, c) * weight(k, rh, rw, c),
            {rh, rw, c}  // reduction axes
        );
        
        return output;
    }
};

// 3. Hardware IR - 硬件指令表示
class HardwareIR {
    enum OpCode {
        LOAD_WEIGHT,    // 加载权重到片上
        LOAD_ACT,       // 加载激活值
        COMPUTE_MAC,    // MAC阵列计算
        STORE_RESULT,   // 存储结果
        SYNC            // 同步指令
    };
    
    struct Instruction {
        OpCode opcode;
        vector<int> operands;
        map<string, int> config;  // 硬件配置参数
    };
};
            </div>
            
            <div class="info-box">
                <p><strong>为什么需要多层IR？</strong></p>
                <ul>
                    <li><strong>Graph IR：</strong>适合做图级别优化，如算子融合、常量折叠、死代码消除</li>
                    <li><strong>Tensor IR：</strong>适合做算子内部优化，如循环变换、向量化、内存访问优化</li>
                    <li><strong>Hardware IR：</strong>贴近硬件，便于指令调度、寄存器分配、硬件特性利用</li>
                </ul>
                <p>现代框架如<strong>MLIR（Multi-Level IR）</strong>提供了构建多层IR的基础设施，被Google、Intel等公司广泛采用于下一代AI编译器。</p>
            </div>

            <h3>10.2 计算图优化</h3>
            
            <p>计算图优化是NPU编译器的核心功能，通过各种变换技术，将原始的计算图转换为更适合硬件执行的形式。</p>

            <h4>10.2.1 算子融合技术</h4>
            <div class="code-block">
// 算子融合示例：Conv + BN + ReLU融合
// 原始计算图
class OriginalGraph:
    def forward(self, x):
        # 卷积操作
        conv_out = self.conv2d(x)  # 需要写回内存
        # 批归一化
        bn_out = self.batch_norm(conv_out)  # 需要读写内存
        # 激活函数
        relu_out = self.relu(bn_out)  # 需要读写内存
        return relu_out

// 融合后的计算图
class FusedGraph:
    def forward(self, x):
        # 融合的算子，一次内存读写完成三个操作
        return self.conv_bn_relu_fused(x)

// 融合实现（伪代码）
def conv_bn_relu_fused(input, conv_weight, bn_params):
    # 在NPU内部完成所有计算
    for (oc in output_channels):
        for (oh, ow in output_positions):
            # 卷积计算
            acc = 0
            for (ic, kh, kw in kernel):
                acc += input[ic][oh+kh][ow+kw] * conv_weight[oc][ic][kh][kw]
            
            # BN计算（在线融合）
            acc = (acc - bn_mean[oc]) / sqrt(bn_var[oc] + eps)
            acc = acc * bn_scale[oc] + bn_bias[oc]
            
            # ReLU计算
            output[oc][oh][ow] = max(0, acc)
    
    return output
            </div>

            <h4>10.2.2 算子融合的类型与限制</h4>
            <div class="code-block">
// 不同类型的算子融合模式
// 1. 垂直融合（Vertical Fusion）- 将element-wise操作融入计算密集型操作
class VerticalFusion {
    // 融合前：Conv -> Add(bias) -> BN -> ReLU
    void unfused_forward(Tensor input) {
        Tensor conv_out = conv2d(input, weight);      // 写回DDR
        Tensor bias_out = add(conv_out, bias);        // 读写DDR
        Tensor bn_out = batch_norm(bias_out);         // 读写DDR
        Tensor relu_out = relu(bn_out);               // 读写DDR
        return relu_out;
    }
    
    // 融合后：所有操作在片上完成
    void fused_forward(Tensor input) {
        // 一次性完成所有计算，只写最终结果
        return conv_bias_bn_relu_fused(input, weight, bias, bn_params);
    }
};

// 2. 水平融合（Horizontal Fusion）- 合并相同类型的并行操作
class HorizontalFusion {
    // 融合前：多个小矩阵乘法分别执行
    void unfused_multi_matmul(vector<Tensor> A_list, vector<Tensor> B_list) {
        vector<Tensor> results;
        for (int i = 0; i < A_list.size(); i++) {
            results.push_back(matmul(A_list[i], B_list[i]));
        }
        return results;
    }
    
    // 融合后：打包成一个大矩阵乘法
    void fused_batched_matmul(vector<Tensor> A_list, vector<Tensor> B_list) {
        Tensor A_packed = pack_tensors(A_list);  // [batch, M, K]
        Tensor B_packed = pack_tensors(B_list);  // [batch, K, N]
        Tensor C_packed = batched_matmul(A_packed, B_packed);
        return unpack_tensors(C_packed);
    }
};

// 3. 融合的限制条件
bool can_fuse(Node* node1, Node* node2) {
    // 检查数据依赖
    if (has_external_dependency(node1, node2)) {
        return false;  // 中间结果被其他节点使用
    }
    
    // 检查内存限制
    size_t fused_memory = estimate_memory(node1) + estimate_memory(node2);
    if (fused_memory > on_chip_memory_size) {
        return false;  // 融合后超出片上内存
    }
    
    // 检查硬件支持
    if (!hardware_supports_fused_op(node1->op_type, node2->op_type)) {
        return false;  // 硬件没有对应的融合指令
    }
    
    // 检查数值稳定性
    if (fusion_affects_numerical_stability(node1, node2)) {
        return false;  // 融合可能导致精度损失
    }
    
    return true;
}
            </div>
            
            <div class="warning-box">
                <p><strong>算子融合的权衡：</strong></p>
                <ul>
                    <li><strong>收益：</strong>减少内存访问、降低带宽压力、减少kernel启动开销</li>
                    <li><strong>代价：</strong>增加代码复杂度、可能降低硬件利用率、限制并行度</li>
                    <li><strong>原则：</strong>优先融合内存受限（memory-bound）的操作，计算受限（compute-bound）的操作谨慎融合</li>
                </ul>
            </div>

            <h4>10.2.3 图优化策略</h4>
            <div class="table-wrapper">
                <table>
                    <thead>
                        <tr>
                            <th>优化技术</th>
                            <th>描述</th>
                            <th>收益</th>
                            <th>适用场景</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>算子融合</td>
                            <td>将多个算子合并为一个</td>
                            <td>减少内存访问</td>
                            <td>连续的element-wise操作</td>
                        </tr>
                        <tr>
                            <td>常量折叠</td>
                            <td>预计算常量表达式</td>
                            <td>减少运行时计算</td>
                            <td>包含常量的子图</td>
                        </tr>
                        <tr>
                            <td>公共子表达式消除</td>
                            <td>复用相同计算结果</td>
                            <td>减少重复计算</td>
                            <td>重复的计算模式</td>
                        </tr>
                        <tr>
                            <td>死代码消除</td>
                            <td>删除无用计算</td>
                            <td>减少计算量</td>
                            <td>条件分支、未使用输出</td>
                        </tr>
                        <tr>
                            <td>布局转换优化</td>
                            <td>优化数据布局</td>
                            <td>提高访存效率</td>
                            <td>不同算子间的数据传递</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <h4>10.2.4 数据布局优化</h4>
            <p>数据布局（Data Layout）对NPU性能影响巨大。不同的布局方式直接影响内存访问模式和计算效率。编译器需要根据硬件特性选择最优布局。</p>
            
            <div class="code-block">
// 常见的数据布局格式
// NCHW vs NHWC 对比示例
class DataLayoutOptimization {
    // NCHW布局：适合传统CPU和某些GPU
    // 内存排列：[batch][channel][height][width]
    // 例：shape=(1,3,224,224)的图像，RGB三通道是连续存储的
    
    // NHWC布局：适合移动端GPU和很多NPU
    // 内存排列：[batch][height][width][channel]
    // 例：shape=(1,224,224,3)的图像，每个像素的RGB是连续的
    
    // 为什么NHWC对某些硬件更友好？
    void convolution_nhwc(float* input, float* weight, float* output) {
        // NHWC布局下，卷积的内层循环访问模式
        for (int h = 0; h < H; h++) {
            for (int w = 0; w < W; w++) {
                for (int oc = 0; oc < OUT_C; oc++) {
                    float sum = 0;
                    for (int kh = 0; kh < KH; kh++) {
                        for (int kw = 0; kw < KW; kw++) {
                            for (int ic = 0; ic < IN_C; ic++) {
                                // 访问是连续的！有利于向量化
                                int in_idx = ((h+kh)*W + (w+kw))*IN_C + ic;
                                int wt_idx = (oc*KH*KW + kh*KW + kw)*IN_C + ic;
                                sum += input[in_idx] * weight[wt_idx];
                            }
                        }
                    }
                    output[(h*W + w)*OUT_C + oc] = sum;
                }
            }
        }
    }
    
    // NPU特定的分块布局（Tiled Layout）
    // 例如：NC/32HW32c - 将通道维度按32分块
    struct TiledTensor {
        // 原始shape: [N, C, H, W]
        // 分块后shape: [N, C/32, H, W, 32]
        // 好处：每个32通道块可以完美映射到SIMD宽度
        
        float* transform_to_tiled(float* nchw_data, int N, int C, int H, int W) {
            int C_outer = (C + 31) / 32;  // 向上取整
            float* tiled = new float[N * C_outer * H * W * 32];
            
            for (int n = 0; n < N; n++) {
                for (int co = 0; co < C_outer; co++) {
                    for (int h = 0; h < H; h++) {
                        for (int w = 0; w < W; w++) {
                            for (int ci = 0; ci < 32; ci++) {
                                int c = co * 32 + ci;
                                if (c < C) {
                                    int src_idx = ((n*C + c)*H + h)*W + w;
                                    int dst_idx = ((((n*C_outer + co)*H + h)*W + w)*32 + ci;
                                    tiled[dst_idx] = nchw_data[src_idx];
                                }
                            }
                        }
                    }
                }
            }
            return tiled;
        }
    };
};

// 编译器的布局选择策略
class LayoutSelector {
    Layout selectOptimalLayout(Graph& graph, NPUTarget& target) {
        // 1. 分析硬件特性
        bool supports_nhwc = target.hasEfficientNHWC();
        int simd_width = target.getSIMDWidth();
        
        // 2. 分析模型特征
        int avg_channel_size = graph.getAverageChannelSize();
        bool has_depthwise = graph.hasDepthwiseConv();
        
        // 3. 决策逻辑
        if (has_depthwise && supports_nhwc) {
            return NHWC;  // Depthwise卷积在NHWC下效率更高
        }
        
        if (avg_channel_size % simd_width == 0) {
            return TiledLayout(simd_width);  // 通道数适合分块
        }
        
        return NCHW;  // 默认布局
    }
};
            </div>
            
            <div class="info-box">
                <p><strong>布局转换的时机：</strong></p>
                <ul>
                    <li><strong>早期转换：</strong>在图优化阶段就确定目标布局，所有算子使用统一布局</li>
                    <li><strong>延迟转换：</strong>让每个算子选择最优布局，必要时插入转换节点</li>
                    <li><strong>混合策略：</strong>将图分区，每个区域使用最适合的布局</li>
                </ul>
            </div>

            <h3>10.3 内存优化技术</h3>
            
            <p>内存带宽是NPU的主要瓶颈。高效的内存管理和优化技术对于发挥NPU性能至关重要。</p>

            <h4>10.3.1 内存分配策略</h4>
            <div class="code-block">
// 内存池管理器
class MemoryPoolManager {
public:
    struct MemoryBlock {
        size_t offset;
        size_t size;
        int lifetime_start;
        int lifetime_end;
        bool is_allocated;
    };
    
    // 内存分配算法
    size_t allocate(size_t size, int start_time, int end_time) {
        // 1. 首先尝试复用已释放的内存块
        for (auto& block : memory_blocks) {
            if (!block.is_allocated && 
                block.size >= size &&
                (block.lifetime_end < start_time || 
                 block.lifetime_start > end_time)) {
                block.is_allocated = true;
                block.lifetime_start = start_time;
                block.lifetime_end = end_time;
                return block.offset;
            }
        }
        
        // 2. 分配新的内存块
        size_t offset = findFreeSpace(size);
        memory_blocks.push_back({
            offset, size, start_time, end_time, true
        });
        
        return offset;
    }
    
    // 内存碎片整理
    void defragment() {
        // 基于生命周期的内存压缩
        std::sort(memory_blocks.begin(), memory_blocks.end(),
            [](const MemoryBlock& a, const MemoryBlock& b) {
                return a.lifetime_start < b.lifetime_start;
            });
        
        // 重新排列内存布局
        size_t current_offset = 0;
        for (auto& block : memory_blocks) {
            if (block.is_allocated) {
                block.offset = current_offset;
                current_offset += block.size;
            }
        }
    }
    
private:
    std::vector<MemoryBlock> memory_blocks;
    size_t total_memory_size;
};
            </div>

            <h4>10.3.2 数据布局优化</h4>
            <div class="info-box">
                <p><strong>常见数据布局格式：</strong></p>
                <ul>
                    <li><strong>NCHW：</strong>批次-通道-高度-宽度，适合卷积运算</li>
                    <li><strong>NHWC：</strong>批次-高度-宽度-通道，适合深度可分离卷积</li>
                    <li><strong>NC/HW[n]c：</strong>分块布局，适合SIMD指令</li>
                    <li><strong>Custom Layout：</strong>NPU特定的优化布局</li>
                </ul>
            </div>

            <div class="code-block">
// 数据布局转换优化
class LayoutOptimizer {
public:
    // 分析最优布局
    Layout analyzeOptimalLayout(const Graph& graph) {
        std::map<Layout, float> layout_costs;
        
        // 遍历所有可能的布局
        for (auto layout : {NCHW, NHWC, NCHW16c, CUSTOM}) {
            float cost = 0;
            
            // 计算每个算子的执行成本
            for (auto& op : graph.operators) {
                cost += getOperatorCost(op, layout);
            }
            
            // 计算布局转换成本
            for (auto& edge : graph.edges) {
                if (requiresTranspose(edge, layout)) {
                    cost += getTransposeCost(edge);
                }
            }
            
            layout_costs[layout] = cost;
        }
        
        // 返回成本最低的布局
        return std::min_element(layout_costs.begin(), layout_costs.end(),
            [](const auto& a, const auto& b) {
                return a.second < b.second;
            })->first;
    }
    
    // 插入布局转换节点
    void insertLayoutTransforms(Graph& graph, const Layout& target_layout) {
        for (auto& edge : graph.edges) {
            if (edge.src_layout != target_layout || 
                edge.dst_layout != target_layout) {
                // 插入转换节点
                auto transform_op = createTransformOp(
                    edge.src_layout, target_layout
                );
                graph.insertOperator(transform_op, edge);
            }
        }
    }
};
            </div>

            <h3>10.4 指令生成与调度</h3>
            
            <p>将优化后的计算图转换为NPU可执行的指令序列，需要考虑指令级并行、数据依赖关系和硬件资源约束。</p>

            <h4>10.4.1 指令调度算法</h4>
            <div class="code-block">
// NPU指令调度器
class InstructionScheduler {
public:
    struct Instruction {
        enum Type { LOAD, STORE, COMPUTE, SYNC };
        Type type;
        int cycle_start;
        int cycle_end;
        std::vector<int> dependencies;
        std::vector<int> resources;  // 使用的硬件资源
    };
    
    // 基于资源约束的指令调度
    std::vector<Instruction> schedule(const std::vector<Instruction>& instructions) {
        // 构建依赖图
        DependencyGraph dep_graph(instructions);
        
        // 初始化就绪队列
        std::priority_queue<int, std::vector<int>, ComparePriority> ready_queue;
        std::vector<bool> scheduled(instructions.size(), false);
        std::vector<int> finish_time(instructions.size(), 0);
        
        // 将无依赖的指令加入就绪队列
        for (int i = 0; i < instructions.size(); i++) {
            if (dep_graph.getPredecessors(i).empty()) {
                ready_queue.push(i);
            }
        }
        
        // 资源使用表
        ResourceTable resource_table;
        int current_cycle = 0;
        
        // 调度主循环
        while (!ready_queue.empty()) {
            int inst_id = ready_queue.top();
            ready_queue.pop();
            
            // 找到最早可以执行的时间
            int earliest_start = current_cycle;
            for (int pred : dep_graph.getPredecessors(inst_id)) {
                earliest_start = std::max(earliest_start, finish_time[pred]);
            }
            
            // 检查资源冲突
            int start_cycle = resource_table.findAvailableSlot(
                instructions[inst_id].resources, 
                earliest_start,
                instructions[inst_id].cycle_end - instructions[inst_id].cycle_start
            );
            
            // 分配资源并调度
            instructions[inst_id].cycle_start = start_cycle;
            instructions[inst_id].cycle_end = start_cycle + 
                (instructions[inst_id].cycle_end - instructions[inst_id].cycle_start);
            
            resource_table.allocate(instructions[inst_id]);
            scheduled[inst_id] = true;
            finish_time[inst_id] = instructions[inst_id].cycle_end;
            
            // 更新就绪队列
            for (int succ : dep_graph.getSuccessors(inst_id)) {
                bool ready = true;
                for (int pred : dep_graph.getPredecessors(succ)) {
                    if (!scheduled[pred]) {
                        ready = false;
                        break;
                    }
                }
                if (ready) {
                    ready_queue.push(succ);
                }
            }
        }
        
        return instructions;
    }
};
            </div>

            <h4>10.4.2 指令级优化</h4>
            <div class="warning-box">
                <p><strong>NPU指令优化技术：</strong></p>
                <ul>
                    <li><strong>指令合并：</strong>将多个简单指令合并为复合指令</li>
                    <li><strong>软件流水线：</strong>重叠不同迭代的指令执行</li>
                    <li><strong>双缓冲：</strong>计算与数据传输并行</li>
                    <li><strong>向量化：</strong>利用SIMD指令处理多个数据</li>
                </ul>
            </div>

            <h3>10.5 自动化编译优化：Triton与TVM</h3>
            
            <p>随着AI硬件的多样化和模型的复杂化，手工优化算子变得越来越困难。Triton和TVM代表了两种不同的自动化编译优化方法，它们通过不同的技术路径帮助开发者实现高效的AI计算。</p>

            <h4>10.5.1 OpenAI Triton：基于Python的GPU编程</h4>
            <div class="info-box">
                <p><strong>Triton的核心理念：</strong></p>
                <ul>
                    <li><strong>高层抽象：</strong>使用Python编写，自动处理底层复杂性</li>
                    <li><strong>块级编程：</strong>以tensor block为基本单位，而非单个线程</li>
                    <li><strong>自动优化：</strong>编译器自动处理内存合并、向量化、线程调度</li>
                    <li><strong>性能便携性：</strong>同一份代码在不同GPU上都能获得优秀性能</li>
                </ul>
            </div>
            
            <div class="code-block">
# Triton编程示例：Fused Attention实现
import triton
import triton.language as tl

@triton.jit
def fused_attention_kernel(
    Q, K, V, Out,
    stride_qm, stride_qk,
    stride_km, stride_kk,
    stride_vm, stride_vk,
    stride_om, stride_ok,
    N, BLOCK_SIZE: tl.constexpr
):
    # 程序块索引
    pid = tl.program_id(0)
    
    # 计算起始位置
    start_m = pid * BLOCK_SIZE
    
    # 加载Q块
    offs_m = start_m + tl.arange(0, BLOCK_SIZE)
    offs_k = tl.arange(0, BLOCK_SIZE)
    
    q = tl.load(Q + offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)
    
    # 初始化累加器
    acc = tl.zeros([BLOCK_SIZE, BLOCK_SIZE], dtype=tl.float32)
    
    # 分块计算QK^T
    for start_n in range(0, N, BLOCK_SIZE):
        # 加载K块
        k = tl.load(K + (start_n + offs_k[:, None]) * stride_km + offs_k[None, :] * stride_kk)
        
        # 计算注意力分数
        qk = tl.dot(q, tl.trans(k))
        qk = qk * (1.0 / tl.sqrt(float(BLOCK_SIZE)))
        
        # Softmax（数值稳定版本）
        qk_max = tl.max(qk, axis=1, keep_dims=True)
        qk = tl.exp(qk - qk_max)
        qk_sum = tl.sum(qk, axis=1, keep_dims=True)
        qk = qk / qk_sum
        
        # 加载V并累加
        v = tl.load(V + (start_n + offs_k[:, None]) * stride_vm + offs_k[None, :] * stride_vk)
        acc += tl.dot(qk, v)
    
    # 存储结果
    tl.store(Out + offs_m[:, None] * stride_om + offs_k[None, :] * stride_ok, acc)

# Triton编译器优化流程
class TritonCompiler:
    def compile(self, kernel_ast):
        # 1. 高层AST分析
        analyzed = self.analyze_memory_access(kernel_ast)
        
        # 2. 内存合并（Memory Coalescing）
        # 自动检测连续内存访问模式
        coalesced = self.coalesce_memory_access(analyzed)
        
        # 3. 向量化（Vectorization）  
        # 将标量操作转换为向量指令
        vectorized = self.vectorize_operations(coalesced)
        
        # 4. 共享内存分配
        # 自动决定哪些数据放入shared memory
        shared_allocated = self.allocate_shared_memory(vectorized)
        
        # 5. 线程调度优化
        # 最大化occupancy和隐藏延迟
        scheduled = self.optimize_thread_schedule(shared_allocated)
        
        # 6. 生成PTX/SASS代码
        return self.generate_gpu_code(scheduled)

# Triton的自动调优机制
@triton.autotune(
    configs=[
        triton.Config({'BLOCK_SIZE': 16}, num_warps=2),
        triton.Config({'BLOCK_SIZE': 32}, num_warps=4),
        triton.Config({'BLOCK_SIZE': 64}, num_warps=8),
        triton.Config({'BLOCK_SIZE': 128}, num_warps=8),
    ],
    key=['M', 'N', 'K'],  # 基于输入大小选择配置
)
@triton.jit
def matmul_kernel(A, B, C, M, N, K, BLOCK_SIZE: tl.constexpr):
    # Triton会自动测试不同配置，选择最佳参数
    pass
            </div>
            
            <div class="warning-box">
                <p><strong>Triton的优势与局限：</strong></p>
                <ul>
                    <li><strong>优势：</strong>
                        <ul>
                            <li>编程模型简单，接近Python/NumPy</li>
                            <li>自动处理GPU编程的复杂细节</li>
                            <li>性能接近手写CUDA</li>
                            <li>代码可读性强，易于维护</li>
                        </ul>
                    </li>
                    <li><strong>局限：</strong>
                        <ul>
                            <li>目前主要支持NVIDIA GPU</li>
                            <li>不适合不规则计算模式</li>
                            <li>编译时间较长</li>
                            <li>调试困难</li>
                        </ul>
                    </li>
                </ul>
            </div>

            <h4>10.5.2 Apache TVM：基于搜索的编译器优化</h4>
            <div class="info-box">
                <p><strong>TVM的核心特性：</strong></p>
                <ul>
                    <li><strong>跨平台：</strong>支持CPU、GPU、NPU等多种硬件</li>
                    <li><strong>自动搜索：</strong>使用机器学习方法搜索最优配置</li>
                    <li><strong>张量表达式：</strong>基于声明式的计算描述</li>
                    <li><strong>模块化设计：</strong>可扩展的编译器架构</li>
                </ul>
            </div>
            
            <div class="code-block">
# TVM编程示例：优化矩阵乘法
import tvm
from tvm import te, auto_scheduler, topi
import numpy as np

# 1. 定义计算（张量表达式）
def matmul_tvm(M, N, K, dtype="float32"):
    # 定义占位符
    A = te.placeholder((M, K), name="A", dtype=dtype)
    B = te.placeholder((K, N), name="B", dtype=dtype)
    
    # 定义约简轴
    k = te.reduce_axis((0, K), name="k")
    
    # 定义计算
    C = te.compute(
        (M, N),
        lambda i, j: te.sum(A[i, k] * B[k, j], axis=k),
        name="C"
    )
    
    return [A, B, C]

# 2. 手动优化策略示例
def manual_schedule_matmul(A, B, C):
    s = te.create_schedule(C.op)
    
    # 获取计算维度
    i, j = C.op.axis
    k = C.op.reduce_axis[0]
    
    # 分块（Tiling）
    TILE_SIZE = 32
    io, ii = s[C].split(i, TILE_SIZE)
    jo, ji = s[C].split(j, TILE_SIZE)
    ko, ki = s[C].split(k, TILE_SIZE)
    
    # 重排序循环
    s[C].reorder(io, jo, ko, ii, ji, ki)
    
    # 向量化
    s[C].vectorize(ji)
    
    # 并行化
    s[C].parallel(io)
    
    # 缓存优化
    AA = s.cache_read(A, "shared", [C])
    BB = s.cache_read(B, "shared", [C])
    CC = s.cache_write(C, "local")
    
    return s

# 3. TVM自动调优（AutoScheduler）
@auto_scheduler.register_workload
def matmul_auto(M, N, K, dtype="float32"):
    return matmul_tvm(M, N, K, dtype)

# 自动搜索最优配置
def auto_tune_matmul(M, N, K, target="cuda"):
    # 定义任务
    task = auto_scheduler.SearchTask(
        func=matmul_auto,
        args=(M, N, K, "float32"),
        target=target
    )
    
    # 配置调优器
    tune_option = auto_scheduler.TuningOptions(
        num_measure_trials=1000,  # 测试次数
        measure_callbacks=[auto_scheduler.RecordToFile("matmul.json")],
        verbose=2,
    )
    
    # 运行自动调优
    task.tune(tune_option)
    
    # 加载最佳配置
    sch, args = task.apply_best("matmul.json")
    
    return sch, args

# 4. TVM的多层优化策略
class TVMOptimizationPipeline:
    def __init__(self, target):
        self.target = target
        
    def optimize(self, compute_dag):
        # 高层图优化
        optimized_graph = self.graph_level_optimization(compute_dag)
        
        # 算子级优化
        scheduled_ops = self.operator_level_optimization(optimized_graph)
        
        # 硬件特定优化
        target_specific = self.hardware_specific_optimization(scheduled_ops)
        
        # 代码生成
        return self.code_generation(target_specific)
    
    def graph_level_optimization(self, graph):
        # 算子融合
        graph = self.fuse_operators(graph)
        
        # 布局转换
        graph = self.optimize_layout(graph)
        
        # 常量折叠
        graph = self.constant_folding(graph)
        
        return graph
    
    def operator_level_optimization(self, graph):
        schedules = {}
        
        for op in graph.operators:
            if self.target == "cuda":
                schedule = self.optimize_for_gpu(op)
            elif self.target == "llvm":
                schedule = self.optimize_for_cpu(op)
            elif "npu" in self.target:
                schedule = self.optimize_for_npu(op)
            
            schedules[op] = schedule
        
        return schedules
    
    def optimize_for_gpu(self, op):
        # GPU特定优化
        schedule = te.create_schedule(op)
        
        # 线程块和线程绑定
        block_x = te.thread_axis("blockIdx.x")
        thread_x = te.thread_axis("threadIdx.x")
        
        # 共享内存使用
        # ...
        
        return schedule
    
    def optimize_for_npu(self, op):
        # NPU特定优化
        schedule = te.create_schedule(op)
        
        # 利用NPU的特殊指令
        if hasattr(op, 'is_depthwise_conv'):
            schedule = self.use_depthwise_instructions(schedule)
        
        # 利用NPU的内存层次
        schedule = self.optimize_memory_hierarchy(schedule)
        
        return schedule

# 5. 基于机器学习的性能模型
class MLBasedTuner:
    def __init__(self):
        # 使用XGBoost构建性能预测模型
        self.performance_model = XGBoostModel()
        self.feature_extractor = FeatureExtractor()
        
    def extract_features(self, schedule):
        """从调度中提取特征"""
        features = {
            'tile_size': schedule.tile_size,
            'unroll_factor': schedule.unroll_factor,
            'vectorize_width': schedule.vectorize_width,
            'parallel_degree': schedule.parallel_degree,
            'memory_access_pattern': self.analyze_memory_pattern(schedule),
            'compute_intensity': self.compute_intensity(schedule),
        }
        return self.feature_extractor.encode(features)
    
    def predict_performance(self, schedule):
        """预测给定调度的性能"""
        features = self.extract_features(schedule)
        return self.performance_model.predict(features)
    
    def search_optimal_schedule(self, search_space, budget=1000):
        """使用贝叶斯优化搜索最优调度"""
        # 初始采样
        initial_samples = self.random_sample(search_space, n=50)
        
        for i in range(budget):
            # 选择下一个试验点
            next_point = self.acquisition_function(search_space)
            
            # 实际测试性能
            actual_perf = self.measure_performance(next_point)
            
            # 更新模型
            self.performance_model.update(next_point, actual_perf)
            
        return self.get_best_schedule()
            </div>
            
            <h4>10.5.3 Triton vs TVM：比较与选择</h4>
            <div class="table-wrapper">
                <table>
                    <thead>
                        <tr>
                            <th>特性</th>
                            <th>Triton</th>
                            <th>TVM</th>
                            <th>适用场景</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>编程模型</td>
                            <td>Python-like，命令式</td>
                            <td>张量表达式，声明式</td>
                            <td>Triton更直观，TVM更灵活</td>
                        </tr>
                        <tr>
                            <td>优化方法</td>
                            <td>编译器自动优化</td>
                            <td>基于搜索的自动调优</td>
                            <td>Triton快速，TVM更精细</td>
                        </tr>
                        <tr>
                            <td>硬件支持</td>
                            <td>主要NVIDIA GPU</td>
                            <td>CPU/GPU/NPU/FPGA</td>
                            <td>TVM更通用</td>
                        </tr>
                        <tr>
                            <td>性能</td>
                            <td>接近手写CUDA</td>
                            <td>取决于调优质量</td>
                            <td>都能达到高性能</td>
                        </tr>
                        <tr>
                            <td>开发效率</td>
                            <td>高，代码简洁</td>
                            <td>中，需要调优时间</td>
                            <td>Triton更适合快速原型</td>
                        </tr>
                        <tr>
                            <td>可扩展性</td>
                            <td>有限</td>
                            <td>高度可扩展</td>
                            <td>TVM适合大规模部署</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            
            <div class="warning-box">
                <p><strong>选择建议：</strong></p>
                <ul>
                    <li><strong>使用Triton：</strong>
                        <ul>
                            <li>主要在NVIDIA GPU上工作</li>
                            <li>需要快速开发自定义算子</li>
                            <li>团队熟悉Python但不熟悉CUDA</li>
                            <li>主要处理规则的张量计算</li>
                        </ul>
                    </li>
                    <li><strong>使用TVM：</strong>
                        <ul>
                            <li>需要跨平台部署</li>
                            <li>目标硬件是NPU或特殊AI加速器</li>
                            <li>愿意投入时间进行性能调优</li>
                            <li>需要精细控制优化过程</li>
                        </ul>
                    </li>
                </ul>
            </div>
            
            <h4>10.5.4 自动化编译优化的未来趋势</h4>
            <div class="info-box">
                <p><strong>发展方向：</strong></p>
                <ol>
                    <li><strong>更智能的自动调优：</strong>
                        <ul>
                            <li>强化学习指导的搜索策略</li>
                            <li>跨模型的经验迁移</li>
                            <li>在线学习和自适应</li>
                        </ul>
                    </li>
                    <li><strong>统一的中间表示：</strong>
                        <ul>
                            <li>MLIR作为通用基础设施</li>
                            <li>更好的前端框架集成</li>
                            <li>模块化和可组合的优化pass</li>
                        </ul>
                    </li>
                    <li><strong>特定领域优化：</strong>
                        <ul>
                            <li>针对Transformer的专门优化</li>
                            <li>稀疏计算的自动优化</li>
                            <li>动态图的高效编译</li>
                        </ul>
                    </li>
                    <li><strong>硬件感知的共同设计：</strong>
                        <ul>
                            <li>编译器与硬件设计的紧密结合</li>
                            <li>可编程的硬件特性</li>
                            <li>软硬件协同优化</li>
                        </ul>
                    </li>
                </ol>
            </div>

            <h3>10.6 量化与精度优化</h3>
            
            <p>量化是将浮点模型转换为低精度定点模型的过程，对于NPU的性能和功耗优化至关重要。现代NPU通常支持INT8甚至INT4计算，相比FP32可以提供4-8倍的性能提升。</p>

            <h4>10.5.1 量化技术分类</h4>
            <div class="info-box">
                <p><strong>主流量化技术对比：</strong></p>
                <table>
                    <tr>
                        <th>量化方法</th>
                        <th>优点</th>
                        <th>缺点</th>
                        <th>适用场景</th>
                    </tr>
                    <tr>
                        <td>训练后量化（PTQ）</td>
                        <td>• 无需重新训练<br>• 快速部署<br>• 只需少量校准数据</td>
                        <td>• 精度损失较大<br>• 对异常值敏感</td>
                        <td>• 精度要求不高的场景<br>• 快速原型验证</td>
                    </tr>
                    <tr>
                        <td>量化感知训练（QAT）</td>
                        <td>• 精度损失小<br>• 可以学习量化友好的权重</td>
                        <td>• 需要完整训练流程<br>• 训练时间长</td>
                        <td>• 高精度要求场景<br>• 量产部署</td>
                    </tr>
                    <tr>
                        <td>混合精度量化</td>
                        <td>• 平衡精度和性能<br>• 灵活性高</td>
                        <td>• 需要逐层分析<br>• 硬件支持复杂</td>
                        <td>• 大型模型<br>• 精度敏感层保护</td>
                    </tr>
                </table>
            </div>

            <h4>10.5.2 训练后量化（PTQ）实现</h4>
            <div class="code-block">
// PTQ实现示例
class PostTrainingQuantization {
public:
    // 校准阶段：收集统计信息
    void calibrate(Model& model, DataLoader& calibration_data) {
        model.eval();  // 设置为评估模式
        
        // 为每层创建统计收集器
        map<string, StatsCollector> layer_stats;
        
        // 运行校准数据
        for (auto& batch : calibration_data) {
            auto activations = model.forward(batch);
            
            // 收集每层的激活值统计
            for (auto& [layer_name, tensor] : activations) {
                layer_stats[layer_name].update(tensor);
            }
        }
        
        // 计算量化参数
        for (auto& [layer_name, stats] : layer_stats) {
            auto quant_params = calculateQuantParams(stats);
            model.setQuantParams(layer_name, quant_params);
        }
    }
    
    // 量化参数计算策略
    QuantParams calculateQuantParams(const StatsCollector& stats) {
        QuantParams params;
        
        if (use_percentile) {
            // 百分位数方法：去除异常值影响
            float min_val = stats.getPercentile(0.1);   // 0.1%
            float max_val = stats.getPercentile(99.9);  // 99.9%
            
            params.scale = (max_val - min_val) / 255.0f;
            params.zero_point = round(-min_val / params.scale);
        } else if (use_mse) {
            // 最小化MSE方法：找到最优的scale和zero_point
            auto [scale, zp] = optimizeQuantParamsMSE(stats.getData());
            params.scale = scale;
            params.zero_point = zp;
        } else {
            // 简单最大最小值方法
            params.scale = (stats.max - stats.min) / 255.0f;
            params.zero_point = round(-stats.min / params.scale);
        }
        
        return params;
    }
    
private:
    // MSE优化：通过网格搜索找到最佳量化参数
    pair<float, int> optimizeQuantParamsMSE(const vector<float>& data) {
        float best_scale = 1.0f;
        int best_zp = 0;
        float min_error = INFINITY;
        
        // 网格搜索
        for (float scale_factor = 0.8f; scale_factor <= 1.2f; scale_factor += 0.02f) {
            float scale = (max_val - min_val) / 255.0f * scale_factor;
            
            for (int zp_offset = -10; zp_offset <= 10; zp_offset++) {
                int zp = base_zero_point + zp_offset;
                
                // 计算量化误差
                float error = 0;
                for (float val : data) {
                    int q = clamp(round(val / scale + zp), 0, 255);
                    float dq = (q - zp) * scale;
                    error += (val - dq) * (val - dq);
                }
                
                if (error < min_error) {
                    min_error = error;
                    best_scale = scale;
                    best_zp = zp;
                }
            }
        }
        
        return {best_scale, best_zp};
    }
};
            </div>

            <h4>10.5.3 量化感知训练（QAT）实现</h4>
            <div class="code-block">
// 量化框架实现
class QuantizationFramework {
public:
    // 对称量化
    struct SymmetricQuantizer {
        float scale;
        int bit_width;
        
        int quantize(float value) {
            int q_max = (1 << (bit_width - 1)) - 1;
            int q_min = -(1 << (bit_width - 1));
            
            int q_value = std::round(value / scale);
            return std::clamp(q_value, q_min, q_max);
        }
        
        float dequantize(int q_value) {
            return q_value * scale;
        }
        
        // 计算量化参数
        void calibrate(const std::vector<float>& values) {
            float max_abs = 0;
            for (float v : values) {
                max_abs = std::max(max_abs, std::abs(v));
            }
            
            int q_max = (1 << (bit_width - 1)) - 1;
            scale = max_abs / q_max;
        }
    };
    
    // 非对称量化
    struct AsymmetricQuantizer {
        float scale;
        int zero_point;
        int bit_width;
        
        int quantize(float value) {
            int q_max = (1 << bit_width) - 1;
            int q_min = 0;
            
            int q_value = std::round(value / scale + zero_point);
            return std::clamp(q_value, q_min, q_max);
        }
        
        float dequantize(int q_value) {
            return (q_value - zero_point) * scale;
        }
        
        // 计算量化参数
        void calibrate(const std::vector<float>& values) {
            float min_val = *std::min_element(values.begin(), values.end());
            float max_val = *std::max_element(values.begin(), values.end());
            
            int q_max = (1 << bit_width) - 1;
            scale = (max_val - min_val) / q_max;
            zero_point = std::round(-min_val / scale);
        }
    };
    
    // 混合精度量化
    void mixedPrecisionQuantize(Graph& graph) {
        // 敏感度分析
        std::map<std::string, float> layer_sensitivity;
        
        for (auto& layer : graph.layers) {
            // 计算每层对精度的敏感度
            float sensitivity = analyzeSensitivity(layer);
            layer_sensitivity[layer.name] = sensitivity;
        }
        
        // 基于敏感度分配位宽
        for (auto& layer : graph.layers) {
            if (layer_sensitivity[layer.name] > 0.9) {
                layer.quantization_bits = 16;  // 高敏感度层使用高精度
            } else if (layer_sensitivity[layer.name] > 0.5) {
                layer.quantization_bits = 8;
            } else {
                layer.quantization_bits = 4;   // 低敏感度层使用低精度
            }
        }
    }
};
            </div>
            
            <h4>10.5.2 低精度浮点格式（FP8/FP4）优化</h4>
            <div class="info-box">
                <h5>新一代低精度浮点量化技术</h5>
                <p>随着大语言模型的兴起，FP8和FP4成为新的量化前沿，在保持浮点数表达能力的同时大幅降低计算和存储成本。</p>
                
                <div class="code-block">
// FP8/FP4 量化器实现
class LowPrecisionFloatQuantizer {
public:
    enum Format {
        FP8_E4M3,    // 1符号位 + 4指数位 + 3尾数位
        FP8_E5M2,    // 1符号位 + 5指数位 + 2尾数位
        FP4_E2M1,    // 1符号位 + 2指数位 + 1尾数位
        FP4_E3M0     // 1符号位 + 3指数位 + 0尾数位
    };
    
    // FP32 到 FP8 转换
    uint8_t quantizeFP8(float value, Format format) {
        if (format == FP8_E4M3) {
            return quantizeFP8_E4M3(value);
        } else if (format == FP8_E5M2) {
            return quantizeFP8_E5M2(value);
        }
        throw std::runtime_error("Unsupported format");
    }
    
private:
    uint8_t quantizeFP8_E4M3(float value) {
        // 特殊值处理
        if (std::isnan(value)) return 0x7F;  // NaN
        if (std::isinf(value)) return value > 0 ? 0x78 : 0xF8;  // ±Inf
        
        // 提取FP32组件
        uint32_t bits = *reinterpret_cast<uint32_t*>(&value);
        uint32_t sign = (bits >> 31) & 0x1;
        uint32_t exp = (bits >> 23) & 0xFF;
        uint32_t mant = bits & 0x7FFFFF;
        
        // 转换到FP8 E4M3
        int32_t exp_fp8 = exp - 127 + 7;  // 重新偏置（FP8偏置=7）
        
        // 范围检查和饱和
        if (exp_fp8 <= 0) {
            // 下溢到0
            return sign << 7;
        } else if (exp_fp8 >= 15) {
            // 上溢到最大值
            return (sign << 7) | 0x77;
        }
        
        // 尾数舍入
        uint32_t mant_fp8 = (mant + 0x100000) >> 20;  // 舍入到3位
        if (mant_fp8 > 7) {
            mant_fp8 = 7;
            exp_fp8++;
        }
        
        return (sign << 7) | (exp_fp8 << 3) | mant_fp8;
    }
    
    uint8_t quantizeFP8_E5M2(float value) {
        // E5M2格式：更大动态范围，适合梯度
        // 实现类似，但指数位更多，尾数位更少
        // ...
    }
};

// FP8/FP4 编译器优化策略
class FP8CompilerOptimizer {
public:
    struct LayerPrecisionConfig {
        Format weight_format;
        Format activation_format;
        Format gradient_format;
    };
    
    // 自动混合精度策略
    std::map<std::string, LayerPrecisionConfig> optimizePrecision(
        const Graph& graph,
        const ProfilingData& profile) {
        
        std::map<std::string, LayerPrecisionConfig> config;
        
        for (const auto& layer : graph.layers) {
            LayerPrecisionConfig layer_config;
            
            // 基于层类型选择格式
            if (layer.type == "MatMul" || layer.type == "Conv") {
                // 权重使用E4M3（更高精度）
                layer_config.weight_format = FP8_E4M3;
                
                // 激活值基于动态范围选择
                float activation_range = profile.getActivationRange(layer.name);
                if (activation_range > 100.0f) {
                    layer_config.activation_format = FP8_E5M2;  // 更大范围
                } else {
                    layer_config.activation_format = FP8_E4M3;  // 更高精度
                }
            } else if (layer.type == "LayerNorm" || layer.type == "BatchNorm") {
                // 归一化层需要更高精度
                layer_config.weight_format = FP8_E4M3;
                layer_config.activation_format = FP8_E4M3;
            } else if (layer.type == "Attention") {
                // 注意力机制的特殊处理
                layer_config.weight_format = FP8_E4M3;
                layer_config.activation_format = FP8_E5M2;  // QK需要大范围
            }
            
            // 梯度始终使用E5M2（防止梯度消失）
            layer_config.gradient_format = FP8_E5M2;
            
            config[layer.name] = layer_config;
        }
        
        return config;
    }
    
    // FP4 极限量化（实验性）
    void extremeQuantizationFP4(Graph& graph) {
        // FP4仅用于特定场景
        for (auto& layer : graph.layers) {
            if (layer.type == "Embedding") {
                // 嵌入层可以使用FP4
                layer.precision = "FP4_E2M1";
            } else if (layer.type == "Dense" && layer.is_output_layer) {
                // 输出层权重可能可以使用FP4
                if (analyzeWeightDistribution(layer) < 0.1) {
                    layer.precision = "FP4_E3M0";
                }
            }
        }
    }
};

// 运行时动态精度切换
class DynamicPrecisionRuntime {
public:
    void executeLowPrecision(const Layer& layer, 
                           const Tensor& input,
                           Tensor& output) {
        // 根据输入动态范围选择精度
        float input_range = input.abs().max().item<float>();
        
        if (input_range < 1.0f && layer.allow_fp4) {
            // 小范围值使用FP4
            executeFP4(layer, input, output);
        } else if (input_range < 100.0f) {
            // 中等范围使用FP8 E4M3
            executeFP8_E4M3(layer, input, output);
        } else {
            // 大范围使用FP8 E5M2
            executeFP8_E5M2(layer, input, output);
        }
    }
    
private:
    void executeFP4(const Layer& layer, const Tensor& input, Tensor& output) {
        // FP4执行路径
        auto input_fp4 = quantizeToFP4(input);
        auto weight_fp4 = layer.weights_fp4;  // 预量化的权重
        
        // 使用专门的FP4硬件单元
        output = npu::fp4_matmul(input_fp4, weight_fp4);
        
        // 累加使用更高精度
        output = accumulateInFP16(output);
    }
};
                </div>
                
                <p><strong>FP8/FP4 最佳实践：</strong></p>
                <ul>
                    <li><strong>格式选择：</strong>权重用E4M3（精度优先），激活/梯度用E5M2（范围优先）</li>
                    <li><strong>累加精度：</strong>即使输入是FP8，累加器应使用FP16或FP32</li>
                    <li><strong>动态切换：</strong>根据数值范围动态选择FP8 vs FP4</li>
                    <li><strong>关键层保护：</strong>LayerNorm、Softmax等敏感层避免过度量化</li>
                    <li><strong>硬件协同：</strong>利用硬件的混合精度矩阵乘法单元</li>
                </ul>
            </div>

            <h4>10.5.3 量化感知训练</h4>
            <div class="code-block">
// 量化感知训练（QAT）实现
class QuantizationAwareTraining {
public:
    // 伪量化操作
    class FakeQuantize : public torch::nn::Module {
    public:
        FakeQuantize(int num_bits, bool symmetric = true) 
            : num_bits_(num_bits), symmetric_(symmetric) {
            if (symmetric_) {
                q_max_ = (1 << (num_bits_ - 1)) - 1;
                q_min_ = -(1 << (num_bits_ - 1));
            } else {
                q_max_ = (1 << num_bits_) - 1;
                q_min_ = 0;
            }
        }
        
        torch::Tensor forward(torch::Tensor x) {
            // 计算scale和zero_point
            torch::Tensor scale, zero_point;
            
            if (symmetric_) {
                auto max_val = x.abs().max();
                scale = max_val / q_max_;
                zero_point = torch::zeros_like(scale);
            } else {
                auto min_val = x.min();
                auto max_val = x.max();
                scale = (max_val - min_val) / (q_max_ - q_min_);
                zero_point = torch::round(-min_val / scale);
            }
            
            // 量化和反量化
            auto x_int = torch::round(x / scale + zero_point);
            x_int = torch::clamp(x_int, q_min_, q_max_);
            auto x_dequant = (x_int - zero_point) * scale;
            
            // 直通估计器（STE）用于反向传播
            return x + (x_dequant - x).detach();
        }
        
    private:
        int num_bits_;
        bool symmetric_;
        float q_max_, q_min_;
    };
    
    // 量化感知的卷积层
    class QuantizedConv2d : public torch::nn::Module {
    public:
        QuantizedConv2d(int in_channels, int out_channels, 
                       int kernel_size, int weight_bits = 8, 
                       int activation_bits = 8) {
            conv_ = torch::nn::Conv2d(
                torch::nn::Conv2dOptions(in_channels, out_channels, kernel_size)
            );
            
            weight_quantizer_ = FakeQuantize(weight_bits, true);
            activation_quantizer_ = FakeQuantize(activation_bits, false);
            
            register_module("conv", conv_);
        }
        
        torch::Tensor forward(torch::Tensor x) {
            // 量化输入激活
            x = activation_quantizer_->forward(x);
            
            // 量化权重
            auto quantized_weight = weight_quantizer_->forward(conv_->weight);
            
            // 执行量化卷积
            return torch::nn::functional::conv2d(
                x, quantized_weight, conv_->bias,
                conv_->options.stride(),
                conv_->options.padding()
            );
        }
        
    private:
        torch::nn::Conv2d conv_{nullptr};
        std::shared_ptr<FakeQuantize> weight_quantizer_;
        std::shared_ptr<FakeQuantize> activation_quantizer_;
    };
};
            </div>

            <h3>10.6 运行时系统设计</h3>
            
            <p>运行时系统（Runtime System）是NPU软件栈的执行引擎，负责将编译器生成的指令序列高效地在硬件上执行。一个优秀的运行时需要处理设备管理、内存管理、任务调度、同步控制等复杂任务。</p>

            <h4>10.6.1 运行时核心职责</h4>
            <div class="info-box">
                <p><strong>NPU运行时的五大核心职责：</strong></p>
                <ol>
                    <li><strong>设备管理：</strong>
                        <ul>
                            <li>NPU硬件初始化和状态管理</li>
                            <li>多设备管理和负载均衡</li>
                            <li>异常处理和错误恢复</li>
                        </ul>
                    </li>
                    <li><strong>内存管理：</strong>
                        <ul>
                            <li>设备内存分配和释放</li>
                            <li>主机-设备数据传输（DMA）</li>
                            <li>内存池管理和碎片整理</li>
                        </ul>
                    </li>
                    <li><strong>任务调度：</strong>
                        <ul>
                            <li>指令序列解析和分发</li>
                            <li>多核/多引擎任务调度</li>
                            <li>依赖关系管理</li>
                        </ul>
                    </li>
                    <li><strong>同步控制：</strong>
                        <ul>
                            <li>主机-设备同步</li>
                            <li>设备内部同步（核间同步）</li>
                            <li>异步执行和事件管理</li>
                        </ul>
                    </li>
                    <li><strong>性能监控：</strong>
                        <ul>
                            <li>执行时间统计</li>
                            <li>资源利用率监控</li>
                            <li>性能瓶颈分析</li>
                        </ul>
                    </li>
                </ol>
            </div>

            <h4>10.6.2 运行时API设计</h4>
            <div class="code-block">
// NPU运行时C++ API示例
class NPURuntime {
public:
    // 1. 设备管理API
    struct DeviceInfo {
        int device_id;
        string device_name;
        size_t memory_size;
        int compute_units;
        float peak_tflops;
    };
    
    static int getDeviceCount();
    static DeviceInfo getDeviceInfo(int device_id);
    void setDevice(int device_id);
    
    // 2. 内存管理API
    class DeviceMemory {
    public:
        void* allocate(size_t size, size_t alignment = 64);
        void free(void* ptr);
        void copyHostToDevice(void* dst, const void* src, size_t size);
        void copyDeviceToHost(void* dst, const void* src, size_t size);
        void copyDeviceToDevice(void* dst, const void* src, size_t size);
        
        // 异步版本
        void copyHostToDeviceAsync(void* dst, const void* src, size_t size, Stream& stream);
    };
    
    // 3. 模型加载与执行API
    class Model {
    public:
        // 加载编译后的模型
        static unique_ptr<Model> load(const string& model_path);
        
        // 获取输入输出信息
        int getNumInputs() const;
        int getNumOutputs() const;
        TensorInfo getInputInfo(int index) const;
        TensorInfo getOutputInfo(int index) const;
        
        // 设置输入
        void setInput(int index, const Tensor& tensor);
        
        // 执行推理
        void run();                    // 同步执行
        Future<void> runAsync();       // 异步执行
        
        // 获取输出
        Tensor getOutput(int index);
    };
    
    // 4. 流（Stream）管理 - 用于异步执行
    class Stream {
    public:
        Stream(int priority = 0);
        
        // 在流中执行任务
        void enqueue(function<void()> task);
        
        // 同步点
        void synchronize();           // 等待流中所有任务完成
        Event recordEvent();          // 记录事件
        void waitEvent(const Event& event);  // 等待事件
    };
    
    // 5. 性能分析API
    class Profiler {
    public:
        void start();
        void stop();
        
        struct ProfileResult {
            string layer_name;
            float time_ms;
            float memory_mb;
            float utilization;
        };
        
        vector<ProfileResult> getResults();
        void exportTrace(const string& filename);  // 导出Chrome Tracing格式
    };
};

// 使用示例
void example_inference() {
    // 1. 初始化运行时
    NPURuntime runtime;
    runtime.setDevice(0);  // 使用第一个NPU设备
    
    // 2. 加载模型
    auto model = NPURuntime::Model::load("resnet50_compiled.npumodel");
    
    // 3. 准备输入数据
    Tensor input_tensor({1, 3, 224, 224}, DataType::FLOAT32);
    fill_input_data(input_tensor);
    
    // 4. 设置输入
    model->setInput(0, input_tensor);
    
    // 5. 执行推理
    model->run();
    
    // 6. 获取输出
    auto output = model->getOutput(0);
    process_output(output);
}

// 异步执行示例
void example_async_inference() {
    NPURuntime runtime;
    auto model = NPURuntime::Model::load("model.npumodel");
    
    // 创建流
    NPURuntime::Stream stream;
    
    // 异步拷贝输入
    runtime.getMemory().copyHostToDeviceAsync(
        device_input, host_input, input_size, stream
    );
    
    // 异步执行
    auto future = model->runAsync();
    
    // 异步拷贝输出
    runtime.getMemory().copyDeviceToHostAsync(
        host_output, device_output, output_size, stream
    );
    
    // 在需要结果时同步
    stream.synchronize();
    
    // 处理结果
    process_output(host_output);
}
            </div>

            <h4>10.6.3 运行时实现架构</h4>
            <div class="code-block">
// NPU运行时系统
class NPURuntime {
public:
    // 执行上下文
    class ExecutionContext {
    public:
        ExecutionContext(NPUDevice* device) : device_(device) {
            // 初始化内存池
            memory_pool_ = std::make_unique<MemoryPool>(
                device->getMemorySize()
            );
            
            // 创建命令队列
            cmd_queue_ = device->createCommandQueue();
            
            // 初始化性能计数器
            perf_counter_ = std::make_unique<PerformanceCounter>();
        }
        
        // 执行推理
        void execute(const CompiledModel& model, 
                    const std::vector<Tensor>& inputs,
                    std::vector<Tensor>& outputs) {
            // 1. 分配输入/输出内存
            auto input_buffers = allocateBuffers(inputs);
            auto output_buffers = allocateBuffers(model.getOutputShapes());
            
            // 2. 拷贝输入数据到设备
            for (size_t i = 0; i < inputs.size(); i++) {
                device_->copyHostToDevice(
                    inputs[i].data(), 
                    input_buffers[i],
                    inputs[i].size()
                );
            }
            
            // 3. 构建执行命令
            CommandBuffer cmd_buffer;
            buildCommandBuffer(cmd_buffer, model, input_buffers, output_buffers);
            
            // 4. 提交执行
            auto fence = cmd_queue_->submit(cmd_buffer);
            
            // 5. 等待完成
            fence->wait();
            
            // 6. 拷贝输出数据
            outputs.resize(output_buffers.size());
            for (size_t i = 0; i < outputs.size(); i++) {
                outputs[i].resize(model.getOutputShapes()[i]);
                device_->copyDeviceToHost(
                    output_buffers[i],
                    outputs[i].data(),
                    outputs[i].size()
                );
            }
            
            // 7. 更新性能统计
            perf_counter_->update(fence->getTimingInfo());
        }
        
    private:
        NPUDevice* device_;
        std::unique_ptr<MemoryPool> memory_pool_;
        std::unique_ptr<CommandQueue> cmd_queue_;
        std::unique_ptr<PerformanceCounter> perf_counter_;
        
        void buildCommandBuffer(CommandBuffer& cmd_buffer,
                              const CompiledModel& model,
                              const std::vector<DeviceBuffer>& inputs,
                              const std::vector<DeviceBuffer>& outputs) {
            // 遍历所有指令
            for (const auto& inst : model.getInstructions()) {
                switch (inst.opcode) {
                    case OPCODE_CONV:
                        cmd_buffer.addConvCommand(inst.params, 
                            inputs[inst.input_idx],
                            outputs[inst.output_idx]);
                        break;
                    
                    case OPCODE_GEMM:
                        cmd_buffer.addGemmCommand(inst.params,
                            inputs[inst.input_idx],
                            outputs[inst.output_idx]);
                        break;
                    
                    case OPCODE_ACTIVATION:
                        cmd_buffer.addActivationCommand(inst.params,
                            inputs[inst.input_idx],
                            outputs[inst.output_idx]);
                        break;
                    
                    case OPCODE_SYNC:
                        cmd_buffer.addSyncCommand();
                        break;
                }
            }
        }
    };
    
    // 多核调度器
    class MultiCoreScheduler {
    public:
        void scheduleSubgraphs(const Graph& graph, 
                             std::vector<NPUCore*>& cores) {
            // 1. 图分割
            auto subgraphs = partitionGraph(graph, cores.size());
            
            // 2. 负载均衡
            balanceWorkload(subgraphs, cores);
            
            // 3. 生成同步点
            insertSynchronization(subgraphs);
            
            // 4. 分配到各个核心
            for (size_t i = 0; i < cores.size(); i++) {
                cores[i]->loadSubgraph(subgraphs[i]);
            }
        }
        
    private:
        std::vector<Subgraph> partitionGraph(const Graph& graph, int num_cores) {
            // 基于最小割的图分割算法
            GraphPartitioner partitioner;
            return partitioner.partition(graph, num_cores);
        }
        
        void balanceWorkload(std::vector<Subgraph>& subgraphs,
                           const std::vector<NPUCore*>& cores) {
            // 估算每个子图的计算量
            std::vector<float> workloads(subgraphs.size());
            for (size_t i = 0; i < subgraphs.size(); i++) {
                workloads[i] = estimateWorkload(subgraphs[i]);
            }
            
            // 动态调整分割边界
            while (!isBalanced(workloads)) {
                adjustPartitionBoundaries(subgraphs, workloads);
            }
        }
    };
};
            </div>

            <h3>10.7 性能分析与调优</h3>
            
            <p>性能分析工具帮助开发者理解模型在NPU上的执行情况，找出性能瓶颈并进行优化。</p>

            <h4>10.7.1 性能分析框架</h4>
            <div class="code-block">
// 性能分析器
class NPUProfiler {
public:
    struct LayerProfile {
        std::string name;
        float compute_time_ms;
        float memory_read_mb;
        float memory_write_mb;
        float utilization;
        std::map<std::string, float> metrics;
    };
    
    // 开始性能分析
    void startProfiling() {
        // 清空之前的数据
        layer_profiles_.clear();
        
        // 启用硬件性能计数器
        enableHardwareCounters();
        
        // 记录开始时间
        start_time_ = getCurrentTime();
        is_profiling_ = true;
    }
    
    // 记录层执行信息
    void recordLayer(const std::string& layer_name,
                    const ExecutionStats& stats) {
        if (!is_profiling_) return;
        
        LayerProfile profile;
        profile.name = layer_name;
        profile.compute_time_ms = stats.compute_cycles / clock_freq_ * 1000;
        profile.memory_read_mb = stats.memory_read_bytes / (1024.0 * 1024.0);
        profile.memory_write_mb = stats.memory_write_bytes / (1024.0 * 1024.0);
        
        // 计算硬件利用率
        profile.utilization = calculateUtilization(stats);
        
        // 收集详细指标
        profile.metrics["mac_efficiency"] = 
            stats.mac_operations / (stats.compute_cycles * max_mac_per_cycle_);
        profile.metrics["memory_bandwidth_utilization"] = 
            (stats.memory_read_bytes + stats.memory_write_bytes) / 
            (stats.compute_cycles / clock_freq_ * memory_bandwidth_);
        profile.metrics["cache_hit_rate"] = 
            stats.cache_hits / float(stats.cache_hits + stats.cache_misses);
        
        layer_profiles_.push_back(profile);
    }
    
    // 生成性能报告
    void generateReport(const std::string& filename) {
        std::ofstream report(filename);
        
        report << "NPU Performance Analysis Report\n";
        report << "================================\n\n";
        
        // 总体统计
        float total_time = 0;
        float total_memory = 0;
        for (const auto& profile : layer_profiles_) {
            total_time += profile.compute_time_ms;
            total_memory += profile.memory_read_mb + profile.memory_write_mb;
        }
        
        report << "Total Execution Time: " << total_time << " ms\n";
        report << "Total Memory Transfer: " << total_memory << " MB\n";
        report << "Average Throughput: " << 
                  (total_ops_ / total_time / 1e6) << " GOPS\n\n";
        
        // 层级详细信息
        report << "Layer-wise Breakdown:\n";
        report << std::setw(30) << "Layer" 
               << std::setw(15) << "Time (ms)"
               << std::setw(15) << "Time %"
               << std::setw(15) << "Utilization"
               << std::setw(20) << "Memory (MB)\n";
        
        for (const auto& profile : layer_profiles_) {
            report << std::setw(30) << profile.name
                   << std::setw(15) << std::fixed << std::setprecision(2) 
                   << profile.compute_time_ms
                   << std::setw(15) << std::fixed << std::setprecision(1)
                   << (profile.compute_time_ms / total_time * 100) << "%"
                   << std::setw(15) << std::fixed << std::setprecision(1)
                   << (profile.utilization * 100) << "%"
                   << std::setw(20) << std::fixed << std::setprecision(2)
                   << (profile.memory_read_mb + profile.memory_write_mb) << "\n";
        }
        
        // 性能瓶颈分析
        report << "\nPerformance Bottlenecks:\n";
        identifyBottlenecks(report);
        
        // 优化建议
        report << "\nOptimization Suggestions:\n";
        generateOptimizationSuggestions(report);
    }
    
private:
    std::vector<LayerProfile> layer_profiles_;
    bool is_profiling_ = false;
    double start_time_;
    float clock_freq_;
    float memory_bandwidth_;
    int max_mac_per_cycle_;
    int64_t total_ops_;
    
    void identifyBottlenecks(std::ofstream& report) {
        // 找出执行时间最长的层
        auto max_time_layer = std::max_element(
            layer_profiles_.begin(), layer_profiles_.end(),
            [](const LayerProfile& a, const LayerProfile& b) {
                return a.compute_time_ms < b.compute_time_ms;
            });
        
        report << "- Slowest layer: " << max_time_layer->name 
               << " (" << max_time_layer->compute_time_ms << " ms)\n";
        
        // 找出内存带宽受限的层
        for (const auto& profile : layer_profiles_) {
            if (profile.metrics.at("memory_bandwidth_utilization") > 0.8) {
                report << "- Memory bandwidth bottleneck in layer: " 
                       << profile.name << "\n";
            }
        }
        
        // 找出利用率低的层
        for (const auto& profile : layer_profiles_) {
            if (profile.utilization < 0.5) {
                report << "- Low utilization in layer: " << profile.name 
                       << " (" << (profile.utilization * 100) << "%)\n";
            }
        }
    }
};
            </div>

            <div class="exercise">
                <h4>练习题集 10</h4>
                
                <div class="question">
                    <p><strong>题目10.1：</strong>解释算子融合的原理，并举例说明Conv+BN+ReLU融合能带来多少内存访问的节省。</p>
                    <details class="hint">
                        <summary>💡 提示</summary>
                        <p>思考方向：算子融合避免了中间结果的存储。分别计算未融合时每个算子的输入输出内存访问量，然后计算融合后的访问量。Conv输出→BN输入、BN输出→ReLU输入这些中间结果可以保持在片上。</p>
                    </details>
                    <button class="toggle-answer">显示答案</button>
                    <div class="answer">
                        <p><strong>答案：</strong></p>
                        <p>算子融合原理：将多个连续的算子合并为一个复合算子，在片上完成所有计算，避免中间结果写回内存。</p>
                        
                        <p>Conv+BN+ReLU融合的内存访问分析：</p>
                        <p>假设特征图大小为H×W×C，数据类型为FP16（2字节）</p>
                        
                        <p><strong>未融合时：</strong></p>
                        <ul>
                            <li>Conv输出写内存：H×W×C×2 字节</li>
                            <li>BN读Conv输出：H×W×C×2 字节</li>
                            <li>BN输出写内存：H×W×C×2 字节</li>
                            <li>ReLU读BN输出：H×W×C×2 字节</li>
                            <li>ReLU输出写内存：H×W×C×2 字节</li>
                            <li>总计：5×H×W×C×2 字节</li>
                        </ul>
                        
                        <p><strong>融合后：</strong></p>
                        <ul>
                            <li>只有最终结果写内存：H×W×C×2 字节</li>
                            <li>节省：80%的内存访问</li>
                        </ul>
                        
                        <p>实际例子：224×224×64的特征图，可节省约24.5MB的内存访问。</p>
                    </div>
                </div>

                <div class="question">
                    <p><strong>题目10.2：</strong>设计一个简单的内存分配算法，支持tensor的生命周期管理和内存复用。</p>
                    <details class="hint">
                        <summary>💡 提示</summary>
                        <p>思考方向：跟踪每个tensor的创建和最后使用时间。使用空闲列表管理可复用的内存块。考虑内存对齐和碎片化问题。可以使用图着色算法来最小化内存使用。</p>
                    </details>
                    <button class="toggle-answer">显示答案</button>
                    <div class="answer">
                        <div class="code-block">
class SimpleMemoryAllocator {
private:
    struct Allocation {
        size_t offset;
        size_t size;
        int start_time;
        int end_time;
    };
    
    size_t total_size;
    std::vector<Allocation> allocations;
    
public:
    SimpleMemoryAllocator(size_t size) : total_size(size) {}
    
    size_t allocate(size_t size, int start, int end) {
        // 按结束时间排序现有分配
        std::sort(allocations.begin(), allocations.end(),
            [](const Allocation& a, const Allocation& b) {
                return a.end_time < b.end_time;
            });
        
        // 尝试找到可复用的内存块
        for (auto& alloc : allocations) {
            if (alloc.end_time <= start && alloc.size >= size) {
                // 复用这块内存
                size_t offset = alloc.offset;
                alloc.start_time = start;
                alloc.end_time = end;
                alloc.size = size;
                return offset;
            }
        }
        
        // 找不到可复用的，分配新的
        size_t offset = 0;
        if (!allocations.empty()) {
            // 找到第一个空闲位置
            std::sort(allocations.begin(), allocations.end(),
                [](const Allocation& a, const Allocation& b) {
                    return a.offset < b.offset;
                });
            
            for (size_t i = 0; i < allocations.size(); i++) {
                if (i == 0 && allocations[i].offset >= size) {
                    offset = 0;
                    break;
                }
                if (i < allocations.size() - 1) {
                    size_t gap_start = allocations[i].offset + allocations[i].size;
                    size_t gap_end = allocations[i+1].offset;
                    if (gap_end - gap_start >= size) {
                        offset = gap_start;
                        break;
                    }
                } else {
                    offset = allocations[i].offset + allocations[i].size;
                }
            }
        }
        
        // 检查是否超出总内存
        if (offset + size > total_size) {
            throw std::runtime_error("Out of memory");
        }
        
        allocations.push_back({offset, size, start, end});
        return offset;
    }
    
    size_t getMaxMemoryUsage() {
        size_t max_usage = 0;
        
        // 对每个时间点计算内存使用
        std::set<int> time_points;
        for (const auto& alloc : allocations) {
            time_points.insert(alloc.start_time);
            time_points.insert(alloc.end_time);
        }
        
        for (int t : time_points) {
            size_t usage = 0;
            for (const auto& alloc : allocations) {
                if (alloc.start_time <= t && t < alloc.end_time) {
                    usage = std::max(usage, alloc.offset + alloc.size);
                }
            }
            max_usage = std::max(max_usage, usage);
        }
        
        return max_usage;
    }
};
                        </div>
                    </div>
                </div>

                <div class="question">
                    <p><strong>题目10.3：</strong>实现一个简单的INT8量化函数，支持对称和非对称量化模式。</p>
                    <details class="hint">
                        <summary>💡 提示</summary>
                        <p>思考方向：对称量化：zero_point=0，范围[-127,127]。非对称量化：有zero_point，范围[0,255]。量化公式：q = round(x/scale) + zero_point。反量化：x = (q - zero_point) * scale。</p>
                    </details>
                    <button class="toggle-answer">显示答案</button>
                    <div class="answer">
                        <div class="code-block">
#include <vector>
#include <algorithm>
#include <cmath>

class INT8Quantizer {
public:
    enum Mode { SYMMETRIC, ASYMMETRIC };
    
private:
    Mode mode;
    float scale;
    int zero_point;
    
public:
    INT8Quantizer(Mode m = SYMMETRIC) : mode(m), scale(1.0f), zero_point(0) {}
    
    // 校准：计算量化参数
    void calibrate(const std::vector<float>& data) {
        if (data.empty()) return;
        
        float min_val = *std::min_element(data.begin(), data.end());
        float max_val = *std::max_element(data.begin(), data.end());
        
        if (mode == SYMMETRIC) {
            // 对称量化：zero_point = 0
            float max_abs = std::max(std::abs(min_val), std::abs(max_val));
            scale = max_abs / 127.0f;
            zero_point = 0;
        } else {
            // 非对称量化
            scale = (max_val - min_val) / 255.0f;
            zero_point = std::round(-min_val / scale);
            zero_point = std::clamp(zero_point, 0, 255);
        }
    }
    
    // 量化单个值
    int8_t quantize(float value) const {
        int quantized = std::round(value / scale + zero_point);
        
        if (mode == SYMMETRIC) {
            return static_cast<int8_t>(std::clamp(quantized, -128, 127));
        } else {
            // 非对称量化结果需要转换到int8范围
            quantized = std::clamp(quantized, 0, 255);
            return static_cast<int8_t>(quantized - 128);
        }
    }
    
    // 反量化
    float dequantize(int8_t qvalue) const {
        if (mode == SYMMETRIC) {
            return qvalue * scale;
        } else {
            // 非对称量化需要先转换回uint8范围
            int value = static_cast<int>(qvalue) + 128;
            return (value - zero_point) * scale;
        }
    }
    
    // 量化向量
    std::vector<int8_t> quantizeVector(const std::vector<float>& input) const {
        std::vector<int8_t> output(input.size());
        for (size_t i = 0; i < input.size(); i++) {
            output[i] = quantize(input[i]);
        }
        return output;
    }
    
    // 计算量化误差
    float calculateError(const std::vector<float>& original) const {
        float total_error = 0;
        for (float val : original) {
            float dequantized = dequantize(quantize(val));
            total_error += std::pow(val - dequantized, 2);
        }
        return std::sqrt(total_error / original.size());
    }
    
    // 获取量化参数
    float getScale() const { return scale; }
    int getZeroPoint() const { return zero_point; }
};

// 使用示例
void testQuantization() {
    std::vector<float> weights = {-1.5, -0.5, 0.0, 0.5, 1.5, 2.0};
    
    // 对称量化
    INT8Quantizer sym_quantizer(INT8Quantizer::SYMMETRIC);
    sym_quantizer.calibrate(weights);
    
    std::cout << "Symmetric Quantization:\n";
    std::cout << "Scale: " << sym_quantizer.getScale() << "\n";
    for (float w : weights) {
        int8_t q = sym_quantizer.quantize(w);
        float dq = sym_quantizer.dequantize(q);
        std::cout << w << " -> " << (int)q << " -> " << dq << "\n";
    }
    
    // 非对称量化
    INT8Quantizer asym_quantizer(INT8Quantizer::ASYMMETRIC);
    asym_quantizer.calibrate(weights);
    
    std::cout << "\nAsymmetric Quantization:\n";
    std::cout << "Scale: " << asym_quantizer.getScale() 
              << ", Zero Point: " << asym_quantizer.getZeroPoint() << "\n";
    for (float w : weights) {
        int8_t q = asym_quantizer.quantize(w);
        float dq = asym_quantizer.dequantize(q);
        std::cout << w << " -> " << (int)q << " -> " << dq << "\n";
    }
}
                        </div>
                    </div>
                </div>

                <div class="question">
                    <p><strong>题目10.4：</strong>设计一个简单的指令调度算法，考虑数据依赖和硬件资源限制。</p>
                    <details class="hint">
                        <summary>💡 提示</summary>
                        <p>思考方向：构建依赖图（DAG）。使用拓扑排序找到可执行指令。考虑资源约束（MAC单元、内存带宽）。优先级可以基于关键路径或资源利用率。</p>
                    </details>
                    <button class="toggle-answer">显示答案</button>
                    <div class="answer">
                        <div class="code-block">
#include <vector>
#include <queue>
#include <unordered_set>

class SimpleScheduler {
public:
    struct Instruction {
        int id;
        std::string type;  // "LOAD", "COMPUTE", "STORE"
        std::vector<int> dependencies;
        int cycles;        // 执行所需周期数
        int resource;      // 所需资源类型
    };
    
    struct ScheduledInst {
        int id;
        int start_cycle;
        int end_cycle;
    };
    
private:
    // 资源使用表
    std::vector<std::vector<bool>> resource_table;
    int num_resources;
    int max_cycles;
    
public:
    SimpleScheduler(int resources, int cycles) 
        : num_resources(resources), max_cycles(cycles) {
        resource_table.resize(resources, std::vector<bool>(cycles, false));
    }
    
    std::vector<ScheduledInst> schedule(const std::vector<Instruction>& instructions) {
        std::vector<ScheduledInst> result;
        std::vector<int> finish_time(instructions.size(), -1);
        std::vector<bool> scheduled(instructions.size(), false);
        
        // 构建依赖关系图
        std::vector<std::vector<int>> dependents(instructions.size());
        std::vector<int> in_degree(instructions.size(), 0);
        
        for (size_t i = 0; i < instructions.size(); i++) {
            in_degree[i] = instructions[i].dependencies.size();
            for (int dep : instructions[i].dependencies) {
                dependents[dep].push_back(i);
            }
        }
        
        // 初始化就绪队列（使用贪心策略：优先调度执行时间长的）
        auto cmp = [&](int a, int b) {
            return instructions[a].cycles < instructions[b].cycles;
        };
        std::priority_queue<int, std::vector<int>, decltype(cmp)> ready_queue(cmp);
        
        // 将无依赖的指令加入就绪队列
        for (size_t i = 0; i < instructions.size(); i++) {
            if (in_degree[i] == 0) {
                ready_queue.push(i);
            }
        }
        
        // 调度主循环
        while (!ready_queue.empty()) {
            int inst_id = ready_queue.top();
            ready_queue.pop();
            
            const auto& inst = instructions[inst_id];
            
            // 计算最早开始时间
            int earliest_start = 0;
            for (int dep : inst.dependencies) {
                if (finish_time[dep] != -1) {
                    earliest_start = std::max(earliest_start, finish_time[dep]);
                }
            }
            
            // 找到可用的资源时间槽
            int start_cycle = findAvailableSlot(inst.resource, 
                                               earliest_start, 
                                               inst.cycles);
            
            if (start_cycle == -1 || start_cycle + inst.cycles > max_cycles) {
                // 资源不足或超出最大周期限制
                continue;
            }
            
            // 分配资源
            for (int c = start_cycle; c < start_cycle + inst.cycles; c++) {
                resource_table[inst.resource][c] = true;
            }
            
            // 记录调度结果
            finish_time[inst_id] = start_cycle + inst.cycles;
            scheduled[inst_id] = true;
            result.push_back({inst_id, start_cycle, start_cycle + inst.cycles});
            
            // 更新依赖关系，将新的就绪指令加入队列
            for (int dependent : dependents[inst_id]) {
                in_degree[dependent]--;
                if (in_degree[dependent] == 0) {
                    ready_queue.push(dependent);
                }
            }
        }
        
        return result;
    }
    
private:
    int findAvailableSlot(int resource, int start_time, int duration) {
        for (int t = start_time; t <= max_cycles - duration; t++) {
            bool available = true;
            for (int d = 0; d < duration; d++) {
                if (resource_table[resource][t + d]) {
                    available = false;
                    break;
                }
            }
            if (available) {
                return t;
            }
        }
        return -1;
    }
};

// 使用示例
void testScheduler() {
    SimpleScheduler scheduler(3, 20);  // 3个资源，20个周期
    
    std::vector<SimpleScheduler::Instruction> instructions = {
        {0, "LOAD", {}, 2, 0},          // 加载数据
        {1, "LOAD", {}, 2, 0},          // 加载权重
        {2, "COMPUTE", {0, 1}, 5, 1},   // 计算，依赖0和1
        {3, "COMPUTE", {0, 1}, 5, 1},   // 并行计算
        {4, "STORE", {2}, 2, 2},        // 存储结果
        {5, "STORE", {3}, 2, 2}         // 存储结果
    };
    
    auto scheduled = scheduler.schedule(instructions);
    
    std::cout << "Scheduled Instructions:\n";
    for (const auto& s : scheduled) {
        std::cout << "Instruction " << s.id 
                  << ": Cycle " << s.start_cycle 
                  << " - " << s.end_cycle << "\n";
    }
}
                        </div>
                    </div>
                </div>

                <div class="question">
                    <p><strong>题目10.5：</strong>分析并优化一个简单的神经网络层在NPU上的内存访问模式。</p>
                    <details class="hint">
                        <summary>💡 提示</summary>
                        <p>思考方向：分析数据复用机会（输入、权重、输出）。考虑tiling策略来适应片上内存大小。计算内存带宽需求和计算强度。优化数据布局减少bank冲突。</p>
                    </details>
                    <button class="toggle-answer">显示答案</button>
                    <div class="answer">
                        <p><strong>答案：</strong></p>
                        <p>以1×1卷积层为例（通常用于通道数变换）：</p>
                        
                        <p><strong>原始实现的内存访问模式：</strong></p>
                        <div class="code-block">
// 朴素实现
for (int n = 0; n < N; n++) {           // batch
    for (int oc = 0; oc < C_out; oc++) { // 输出通道
        for (int h = 0; h < H; h++) {     // 高度
            for (int w = 0; w < W; w++) { // 宽度
                float sum = 0;
                for (int ic = 0; ic < C_in; ic++) { // 输入通道
                    sum += input[n][ic][h][w] * weight[oc][ic];
                }
                output[n][oc][h][w] = sum;
            }
        }
    }
}

// 内存访问分析：
// - Input: 每个元素被读取C_out次
// - Weight: 每个元素被读取N×H×W次
// - 缓存不友好：跳跃式访问input的通道维度
                        </div>
                        
                        <p><strong>优化后的实现：</strong></p>
                        <div class="code-block">
// 优化1：循环重排序，提高数据局部性
for (int n = 0; n < N; n++) {
    for (int h = 0; h < H; h++) {
        for (int w = 0; w < W; w++) {
            // 将输入数据加载到本地缓存
            float local_input[C_in];
            for (int ic = 0; ic < C_in; ic++) {
                local_input[ic] = input[n][ic][h][w];
            }
            
            // 计算所有输出通道
            for (int oc = 0; oc < C_out; oc++) {
                float sum = 0;
                // 向量化计算
                #pragma unroll 8
                for (int ic = 0; ic < C_in; ic++) {
                    sum += local_input[ic] * weight[oc][ic];
                }
                output[n][oc][h][w] = sum;
            }
        }
    }
}

// 优化2：分块（tiling）处理
const int TILE_H = 8, TILE_W = 8, TILE_OC = 32;

for (int n = 0; n < N; n++) {
    for (int h_tile = 0; h_tile < H; h_tile += TILE_H) {
        for (int w_tile = 0; w_tile < W; w_tile += TILE_W) {
            for (int oc_tile = 0; oc_tile < C_out; oc_tile += TILE_OC) {
                // 预加载权重到片上缓存
                float local_weight[TILE_OC][C_in];
                for (int oc = 0; oc < TILE_OC && oc_tile + oc < C_out; oc++) {
                    for (int ic = 0; ic < C_in; ic++) {
                        local_weight[oc][ic] = weight[oc_tile + oc][ic];
                    }
                }
                
                // 处理tile内的计算
                for (int h = 0; h < TILE_H && h_tile + h < H; h++) {
                    for (int w = 0; w < TILE_W && w_tile + w < W; w++) {
                        // 加载输入到寄存器
                        float local_input[C_in];
                        for (int ic = 0; ic < C_in; ic++) {
                            local_input[ic] = input[n][ic][h_tile + h][w_tile + w];
                        }
                        
                        // 计算输出
                        for (int oc = 0; oc < TILE_OC && oc_tile + oc < C_out; oc++) {
                            float sum = 0;
                            for (int ic = 0; ic < C_in; ic++) {
                                sum += local_input[ic] * local_weight[oc][ic];
                            }
                            output[n][oc_tile + oc][h_tile + h][w_tile + w] = sum;
                        }
                    }
                }
            }
        }
    }
}
                        </div>
                        
                        <p><strong>优化效果分析：</strong></p>
                        <ul>
                            <li>输入数据复用：从C_out次降低到C_out/TILE_OC次</li>
                            <li>权重数据复用：在每个tile内复用TILE_H×TILE_W次</li>
                            <li>缓存友好：连续访问，提高缓存命中率</li>
                            <li>向量化友好：内层循环可以SIMD并行</li>
                        </ul>
                    </div>
                </div>
                
                <div class="question">
                    <p><strong>题目10.6：</strong>设计一个NPU软件栈的架构，包含从AI框架到硬件指令的完整流程。说明每层的主要功能和接口。</p>
                    <details class="hint">
                        <summary>💡 提示</summary>
                        <p>思考方向：参考编译器的分层设计。前端解析框架模型，中间表示进行各种优化，后端生成目标代码。每层之间通过定义良好的IR传递信息。</p>
                    </details>
                    <button class="toggle-answer">显示答案</button>
                    <div class="answer">
                        <p><strong>答案：</strong></p>
                        <p>NPU软件栈架构设计：</p>
                        
                        <div class="code-block">
// 1. 框架适配层
class FrameworkAdapter {
    // 支持TensorFlow、PyTorch、ONNX等
    Graph parseModel(string model_path) {
        if (model_path.endswith(".pb")) {
            return parseTensorFlow(model_path);
        } else if (model_path.endswith(".onnx")) {
            return parseONNX(model_path);
        }
        // 统一转换为内部图表示
    }
};

// 2. 高层IR（图级优化）
class HighLevelOptimizer {
    void optimize(Graph& graph) {
        // 算子融合：Conv+BN+ReLU
        fuseOperators(graph);
        // 常量折叠
        foldConstants(graph);
        // 死代码消除
        eliminateDeadCode(graph);
        // 代数简化
        algebraicSimplification(graph);
    }
};

// 3. 中层IR（算子级优化）
class OperatorOptimizer {
    void optimize(Graph& graph) {
        // 数据布局优化
        optimizeDataLayout(graph);
        // Tiling策略
        applyTiling(graph);
        // 并行化分析
        analyzeParallelism(graph);
    }
};

// 4. 低层IR（指令生成）
class CodeGenerator {
    Program generate(Graph& graph) {
        // 内存分配
        allocateMemory(graph);
        // 指令调度
        scheduleInstructions(graph);
        // 寄存器分配
        allocateRegisters(graph);
        // 生成二进制代码
        return generateBinary(graph);
    }
};
                        </div>
                    </div>
                </div>
                
                <div class="question">
                    <p><strong>题目10.7：</strong>比较训练后量化（PTQ）和量化感知训练（QAT）的优缺点，并给出选择建议。</p>
                    <details class="hint">
                        <summary>💡 提示</summary>
                        <p>思考方向：PTQ不需要重新训练，但精度损失可能较大。QAT需要重新训练，但可以获得更好的精度。考虑部署场景、精度要求、计算资源等因素。</p>
                    </details>
                    <button class="toggle-answer">显示答案</button>
                    <div class="answer">
                        <p><strong>答案：</strong></p>
                        <p>PTQ vs QAT比较：</p>
                        
                        <table>
                            <tr>
                                <th>特性</th>
                                <th>训练后量化（PTQ）</th>
                                <th>量化感知训练（QAT）</th>
                            </tr>
                            <tr>
                                <td>实现复杂度</td>
                                <td>简单，不需要修改训练代码</td>
                                <td>复杂，需要修改训练流程</td>
                            </tr>
                            <tr>
                                <td>时间成本</td>
                                <td>快速，只需要校准数据集</td>
                                <td>慢，需要完整训练过程</td>
                            </tr>
                            <tr>
                                <td>精度损失</td>
                                <td>较大，特别是低比特量化</td>
                                <td>较小，模型学习适应量化</td>
                            </tr>
                            <tr>
                                <td>适用场景</td>
                                <td>8bit量化、精度要求不高</td>
                                <td>4bit或更低、精度要求高</td>
                            </tr>
                            <tr>
                                <td>硬件要求</td>
                                <td>推理硬件即可</td>
                                <td>需要训练硬件（GPU）</td>
                            </tr>
                        </table>
                        
                        <p><strong>选择建议：</strong></p>
                        <ul>
                            <li>ResNet等CNN网络：PTQ通常足够（8bit精度损失<1%）</li>
                            <li>MobileNet等轻量网络：建议QAT（对量化更敏感）</li>
                            <li>Transformer类模型：混合策略（权重PTQ，激活值QAT）</li>
                            <li>部署受限场景（4bit或更低）：必须使用QAT</li>
                        </ul>
                    </div>
                </div>
                
                <div class="question">
                    <p><strong>题目10.8：</strong>实现一个简单的内存访问优化算法，通过数据重排减少cache miss。</p>
                    <details class="hint">
                        <summary>💡 提示</summary>
                        <p>思考方向：分析数据访问模式，识别stride访问。通过数据重排让访问变为连续。考虑cache line大小（通常64字节）。使用循环交换、数据打包等技术。</p>
                    </details>
                    <button class="toggle-answer">显示答案</button>
                    <div class="answer">
                        <p><strong>答案：</strong></p>
                        <div class="code-block">
// 内存访问优化器
class MemoryAccessOptimizer {
    // 分析访问模式
    struct AccessPattern {
        int stride;         // 访问步长
        int frequency;      // 访问频率
        int data_size;      // 数据大小
        bool is_sequential; // 是否顺序访问
    };
    
    // 数据重排算法
    void optimizeDataLayout(float* data, int N, int C, int H, int W) {
        // 原始布局：NCHW
        // 如果发现频繁访问不同通道的相同位置
        // 转换为NHWC布局
        
        float* temp = new float[N * C * H * W];
        
        // NCHW -> NHWC转换
        for (int n = 0; n < N; n++) {
            for (int h = 0; h < H; h++) {
                for (int w = 0; w < W; w++) {
                    for (int c = 0; c < C; c++) {
                        // 新索引计算
                        int old_idx = n*C*H*W + c*H*W + h*W + w;
                        int new_idx = n*H*W*C + h*W*C + w*C + c;
                        temp[new_idx] = data[old_idx];
                    }
                }
            }
        }
        
        memcpy(data, temp, N*C*H*W*sizeof(float));
        delete[] temp;
    }
    
    // Cache预取优化
    void insertPrefetch(void* addr, int distance) {
        #ifdef __x86_64__
        __builtin_prefetch(addr + distance, 0, 3);
        #endif
    }
    
    // 循环分块优化
    void tiledAccess(float* matrix, int M, int N) {
        const int TILE = 64;  // Cache line size
        
        for (int i0 = 0; i0 < M; i0 += TILE) {
            for (int j0 = 0; j0 < N; j0 += TILE) {
                // 在tile内顺序访问
                for (int i = i0; i < min(i0+TILE, M); i++) {
                    for (int j = j0; j < min(j0+TILE, N); j++) {
                        process(matrix[i*N + j]);
                    }
                }
            }
        }
    }
};
                        </div>
                    </div>
                </div>
                
                <div class="question">
                    <p><strong>题目10.9：</strong>设计一个NPU运行时的API，支持模型加载、输入输出管理、异步执行等功能。</p>
                    <details class="hint">
                        <summary>💡 提示</summary>
                        <p>思考方向：参考TensorRT、ONNX Runtime等推理框架的API设计。考虑易用性、性能、错误处理。支持批处理、流式执行、多模型并发等高级特性。</p>
                    </details>
                    <button class="toggle-answer">显示答案</button>
                    <div class="answer">
                        <p><strong>答案：</strong></p>
                        <div class="code-block">
// NPU运行时API设计
class NPURuntime {
public:
    // 初始化运行时
    static NPURuntime* create(const Config& config) {
        auto runtime = new NPURuntime();
        runtime->initialize(config);
        return runtime;
    }
    
    // 模型管理
    class Model {
    public:
        // 从文件加载模型
        static Model* load(const string& path, 
                          const ModelConfig& config = {}) {
            auto model = new Model();
            model->loadFromFile(path);
            model->compile(config);
            return model;
        }
        
        // 获取输入输出信息
        vector<TensorInfo> getInputs() const;
        vector<TensorInfo> getOutputs() const;
        
        // 创建执行会话
        Session* createSession(const SessionConfig& config = {});
    };
    
    // 执行会话
    class Session {
    public:
        // 同步执行
        Status run(const vector<Tensor>& inputs,
                  vector<Tensor>& outputs) {
            // 验证输入
            if (!validateInputs(inputs)) {
                return Status::INVALID_INPUT;
            }
            
            // 分配输出
            allocateOutputs(outputs);
            
            // 执行推理
            return executeSync(inputs, outputs);
        }
        
        // 异步执行
        Future<Status> runAsync(const vector<Tensor>& inputs,
                               vector<Tensor>& outputs) {
            return std::async(std::launch::async, [=]() {
                return run(inputs, outputs);
            });
        }
        
        // 批处理执行
        Status runBatch(const vector<vector<Tensor>>& batch_inputs,
                       vector<vector<Tensor>>& batch_outputs) {
            // 动态批处理优化
            return executeBatch(batch_inputs, batch_outputs);
        }
        
        // 流式执行
        class Stream {
            queue<Task> task_queue;
            thread worker;
            
        public:
            void enqueue(const vector<Tensor>& inputs,
                        function<void(vector<Tensor>&)> callback) {
                task_queue.push({inputs, callback});
            }
        };
    };
    
    // 内存管理
    class MemoryPool {
    public:
        Tensor allocate(const TensorInfo& info) {
            return Tensor(allocateBuffer(info.size), info);
        }
        
        void deallocate(Tensor& tensor) {
            releaseBuffer(tensor.data());
        }
    };
    
    // 性能分析
    class Profiler {
    public:
        void start() { profiling = true; }
        void stop() { profiling = false; }
        Report getReport() const;
    };
};

// 使用示例
void example() {
    // 创建运行时
    auto runtime = NPURuntime::create({
        .device_id = 0,
        .memory_limit = 4_GB
    });
    
    // 加载模型
    auto model = runtime->loadModel("model.npubin", {
        .optimization_level = 3,
        .precision = Precision::INT8
    });
    
    // 创建会话
    auto session = model->createSession({
        .max_batch_size = 32,
        .enable_profiling = true
    });
    
    // 准备输入
    vector<Tensor> inputs = {
        runtime->allocate({1, 3, 224, 224})
    };
    
    // 执行推理
    vector<Tensor> outputs;
    auto status = session->run(inputs, outputs);
    
    if (status == Status::SUCCESS) {
        // 处理输出
        processResults(outputs);
    }
}
                        </div>
                    </div>
                </div>
                
                <div class="question">
                    <p><strong>题目10.10：</strong>分析NCHW和NHWC两种数据布局在NPU上的性能差异，并给出选择建议。</p>
                    <details class="hint">
                        <summary>💡 提示</summary>
                        <p>思考方向：NCHW适合卷积的向量化计算，NHWC适合depthwise卷积。考虑不同算子的访问模式、向量化能力、内存带宽利用率。现代NPU可能有专门的布局转换单元。</p>
                    </details>
                    <button class="toggle-answer">显示答案</button>
                    <div class="answer">
                        <p><strong>答案：</strong></p>
                        <p>NCHW vs NHWC性能分析：</p>
                        
                        <table>
                            <tr>
                                <th>算子类型</th>
                                <th>NCHW性能</th>
                                <th>NHWC性能</th>
                                <th>原因分析</th>
                            </tr>
                            <tr>
                                <td>标准卷积</td>
                                <td>优秀</td>
                                <td>良好</td>
                                <td>NCHW可以向量化处理整个通道</td>
                            </tr>
                            <tr>
                                <td>Depthwise卷积</td>
                                <td>较差</td>
                                <td>优秀</td>
                                <td>NHWC连续访问同一位置的所有通道</td>
                            </tr>
                            <tr>
                                <td>BatchNorm</td>
                                <td>良好</td>
                                <td>一般</td>
                                <td>NCHW便于收集通道统计信息</td>
                            </tr>
                            <tr>
                                <td>池化操作</td>
                                <td>优秀</td>
                                <td>良好</td>
                                <td>两种布局差异不大</td>
                            </tr>
                            <tr>
                                <td>Element-wise</td>
                                <td>相同</td>
                                <td>相同</td>
                                <td>顺序访问，无差异</td>
                            </tr>
                        </table>
                        
                        <p><strong>NPU实现考虑：</strong></p>
                        <div class="code-block">
// 布局感知的算子实现
class LayoutAwareConvolution {
    void execute(Tensor input, Layout layout) {
        if (layout == NCHW) {
            // SIMD处理整个输出通道
            for (int oc = 0; oc < C_out; oc += SIMD_WIDTH) {
                // 向量化计算
                computeNCHW_SIMD(input, oc);
            }
        } else { // NHWC
            // 适合MAC阵列的数据流
            for (int h = 0; h < H; h++) {
                for (int w = 0; w < W; w++) {
                    // 所有通道并行计算
                    computeNHWC_Parallel(input, h, w);
                }
            }
        }
    }
};
                        </div>
                        
                        <p><strong>选择建议：</strong></p>
                        <ul>
                            <li>ResNet类网络：NCHW（标准卷积为主）</li>
                            <li>MobileNet类网络：NHWC（大量depthwise卷积）</li>
                            <li>混合网络：动态转换或使用NC/HW[n]c分块布局</li>
                            <li>硬件特定：根据NPU的MAC阵列设计选择</li>
                            <li>带宽受限场景：选择减少内存访问的布局</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>
        
        <!-- Chapter 11: Performance Optimization Techniques -->
        </div>
        
        <div class="chapter-nav">
            <a href="chapter9.html" class="prev">上一章</a>
            <a href="chapter11.html" class="next">下一章</a>
        </div>
    </div>
</body>
</html>